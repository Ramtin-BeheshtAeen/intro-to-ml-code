\documentclass{article}
\usepackage[a4paper, top=2cm,right=1cm,left=1cm,bottom=0.5cm]{geometry}
\usepackage{url}

\usepackage{ifthen}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{tikz} 

\usepackage{multicol}
\usepackage[compact]{titlesec}

\usepackage{mathtools}
\usepackage{siunitx}
\sisetup{inter-unit-product=\ensuremath{{}\cdot{}}, exponent-product = \cdot}
\DeclareSIUnit\bar{bar}

\usepackage[version=4]{mhchem} 

\makeatletter
\newsavebox\myboxA
\newsavebox\myboxB
\newlength\mylenA
\newcommand*\overbar[2][0.75]{%
    \sbox{\myboxA}{$\m@th#2$}%
    \setbox\myboxB\null% Phantom box
    \ht\myboxB=\ht\myboxA%
    \dp\myboxB=\dp\myboxA%
    \wd\myboxB=#1\wd\myboxA% Scale phantom
    \sbox\myboxB{$\m@th\overline{\copy\myboxB}$}%  Overlined phantom
    \setlength\mylenA{\the\wd\myboxA}%   calc width diff
    \addtolength\mylenA{-\the\wd\myboxB}%
    \ifdim\wd\myboxB<\wd\myboxA%
       \rlap{\hskip 0.5\mylenA\usebox\myboxB}{\usebox\myboxA}%
    \else
        \hskip -0.5\mylenA\rlap{\usebox\myboxA}{\hskip 0.5\mylenA\usebox\myboxB}%
    \fi}
\makeatother
\renewcommand{\arraystretch}{1.2}

\makeatletter
\renewcommand*\env@matrix[1][\arraystretch]{%
  \edef\arraystretch{#1}%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{*\c@MaxMatrixCols c}}
\makeatother

\usepackage{esint}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{enumitem}

\usepackage{float}
\usepackage{fancyhdr}
\usepackage{lastpage,refcount, atbegshi}
\usepackage[hidelinks]{hyperref}
\AtBeginShipout{%
  \ifnum\value{page}=\number\numexpr\getpagerefnumber{LastPage}-2\relax
    \phantomsection\label{preLastPage}
  \fi}

% \usepackage{polyglossia}

% \setmainfont{David CLM}
% \setsansfont{Miriam CLM}

% \setdefaultlanguage{english}
% \setotherlanguage{english}  

% \setmathrm{TeX Gyre Schola}

\makeatletter
\def\xpg@set@normalfont#1{%
  \letcs{\rmfamily}{#1@font@rm}%
  \letcs{\sffamily}{#1@font@sf}%
  \letcs{\ttfamily}{#1@font@tt}%
  \def\normalfont{\protect\xpg@select@fontfamily{#1}}%def instead of gdef
  \gdef\reset@font{\protect\normalfont}%
}
% \addto\inlineextras@english{\xpg@set@normalfont{english}}
% \addto\blockextras@english{\xpg@set@normalfont{english}}
\makeatother

\usepackage[style=english]{csquotes}

\usepackage[continuous]{pagenote}
\renewcommand*{\notesname}{NOTE NAME}
\renewcommand{\sectionname}{SECTION NAME}
\renewcommand{\notenuminnotes}[1]{{\textnormal#1. }}
\makepagenote

\renewcommand\text[1]{\textnormal{\textenglish{#1}}}

\setlength{\parindent}{0pt}
\pagestyle{fancy}     % Set the page style to fancy
\fancyhf{}            % Clear all header and footer fields
\renewcommand{\headrulewidth}{0pt} % No line in header
\setlength{\headsep}{0.2cm}% Set the space between header and text

% Define header content
\fancyhead[L]{Page: {\thepage}} % Left header
\fancyhead[R]{\ifthenelse{\value{page}=1}{\today}{} } % Right header shows date on first page

\def\imagewidth{0.9}

\newenvironment{cheatformula}[1][כותרת]{
    \begin{minipage}{\linewidth}
    \textbf{#1}:
}{
    % changing vertical  space between every cheat formula
    \end{minipage}\\[0ex]
}

\newcommand{\cheatimage}[4][\imagewidth]{
    \begin{figure}[H]
        \centering
        \includegraphics[width=#1\linewidth]{#2}
        \caption{#3}
        \label{#4}
    \end{figure}
}

\newcommand*{\NameAndID}{%
    \par\noindent\makebox[2in]{\hrulefill} \hspace{0.5in}\makebox[2.0in]{\hrulefill}%
    \par\noindent\makebox[2in][r]{Ramtin Behesht Aeen}      \hspace{0.5in}\makebox[2.0in][r]{ramtinba145822@gmail.com}%
}%

\title{Summary of }
\author{Auther}

\begin{document}

\setlength{\columnsep}{1cm}
\setlength{\columnseprule}{1pt}

\makeatletter
\begin{center}
    {\NameAndID}\\[2ex]
    {\Large{\textbf{\@title}}}\\[2ex]
\end{center}
\makeatother

\begin{multicols*}{2}
\raggedcolumns

\section{Cost Function for one Dimension}

\begin{cheatformula}[Learning Model]

\begin{itemize}
  \item[$\circ$] Linear Regression Hypothesis:
  $$ \mathbf{h_{w}X} = w_{0} + w_{1} x_{1} + ... + w_{D}x_{D} := \mathbf{W^{T}X}$$
  \item[$\circ$] Input Vector X :
  $$ X= [x_{0}=1, \, x_{1}, \, x_{2}, \, ... \, ,x_{D}] $$
  \item[$\circ$] Parameter Vector W (features) :
  $$ W= [w_{0}=1, \, w_{1}, \, w_{2}, \, ... \, ,w_{D}] $$
\end{itemize}
\end{cheatformula}

\begin{cheatformula}[Squared Error(SE)] 
Most Common error function in linear regression is:
$$SE: ( y^{(i)} - h(x^{(i)}, w))^{2}$$
\end{cheatformula}


\begin{cheatformula}[Sum of Squared Errors (SSE)]
Cost function should measure all predictions. Thus a choice could be Sum of Squared Error(SE)
$$ SSE: \sum_{i=1}^{N}(y^{(i)} - h(x^{(i)}, w) ) $$
\end{cheatformula}

\begin{cheatformula}[Solve it analytically for one dimension]
Predicted:
$$  \widehat{y} = w_{0} + w_{1}x $$
SSE or Cost Function:
$$ J(w_{0}, w_{1}) := \sum_{i=1}^{N}(y^{(i)} - \widehat{y}^{(i)})^{2} = \sum_{i=1}^{N}(y^{(i)} -  w_{0} + w_{1}x^{(i)})^{2} $$
Assumptions:
$$\frac{\partial J}{\partial w_{0}} = 0 \,, \frac{\partial J}{\partial w_{1}} = 0$$
$\frac{\partial J}{\partial w_{0}} = 0:$ thus:
 $$ \frac{\partial }{\partial w_{0}}  ( \sum_{i=1}^{N}(y^{(i)} -  (w_{0} + w_{1}x^{(i)}))^{2}) = 0  \, \footnotemark $$

 $$ -2 \sum_{i=1}^{N}(y^{(i)} -  w_{0} - w_{1}x^{(i)}) = 0 $$
 \footnotetext{\textsuperscript{\thefootnote} $fog(x)' = f(g(x))' g(x)'$}

For this equation to equal zero, the following condition must be met:
\begin{itemize}
\item[$\circ$] $\sum_{i=1}^{N}y^{(i)} = 0 := Y$ 
\item[$\circ$] $\sum_{i=1}^{N}-w_{0}  = 0 := n w_{0} $ 
\item[$\circ$] $\sum_{i=1}^{N}-w_{1}x^{(i)}  = 0 := X $ 
\end{itemize}
Thus:
$$ 0 = Y - n w_{0} -w_{1}x^{(i)} \longrightarrow  w_{0} = \frac{(Y -w_{1}x^{(i)})}{n} $$
\end{cheatformula}


\begin{cheatformula}[Second Part $\frac{\partial J}{\partial w_{1}} = 0$ ]\\

$$ \frac{\partial }{\partial w_{1}}  ( \sum_{i=1}^{N}(y^{(i)} -  (w_{0} + w_{1}x^{(i)}))^{2}) = 0   $$

$$  \sum_{i=1}^{N} 2(y^{(i)} -  (w_{0} + w_{1}x^{(i)})) x^{(i)} = 0  \, \footnotemark $$

$$  \sum_{i=1}^{N} (x^{(i)}y^{(i)} - w_{0}x^{(i)} + w_{1}x^{(i)^{2}} = 0 $$

$ \sum_{i=1}^{N} (x^{(i)})=X   ,\,  w_{0} = \frac{(Y -w_{1}x^{(i)})}{n} \, so: $

$$  \sum_{i=1}^{N} (x^{(i)}y^{(i)} - \frac{(Y -w_{1}x^{(i)})}{n} X  + w_{1}x^{(i)^{2}} = 0 $$

$$  n \sum_{i=1}^{N} x^{(i)}y^{(i)} - {(YX -w_{1}X^{2})}   + n\sum_{i=1}^{N}w_{1}x^{(i)^{2}} = 0 $$

$$  n \sum_{i=1}^{N} x^{(i)}y^{(i)} - YX=  - w_{1}X^{2}   + n w_{1}\sum_{i=1}^{N}x^{(i)^{2}}  $$

$$n \sum_{i=1}^{N} x^{(i)}y^{(i)} - YX = w_{1} (n \sum_{i=1}^{N}x^{(i)^{2}} - X^{2})$$

$$w_{1} = \frac{n \sum_{i=1}^{N} x^{(i)}y^{(i)} - YX}{n \sum_{i=1}^{N}x^{(i)^{2}} - X^{2}}$$

 \footnotetext{\textsuperscript{\thefootnote} $ \frac{\partial}  {\partial x} xy =y \frac{\partial}  {\partial x}x =y $}
 
\end{cheatformula}
\pagebreak


\section{Cost Function for two oe more Dimension}

\begin{cheatformula}[Analytical Solution for D Dimensions:]\
In this section, the solution for D dimensions is discussed. For example, in apartment price prediction, we cannot only consider the size, but also other features like the year it was built, the number of rooms, and other property characteristics. Therefore, our x vector will contain multiple properties.

$$x ^{(1)}= \begin{pmatrix}
 1\\ 3 \\1390 \\80
\end{pmatrix}$$

For example in this three dimension, here 1 represents the intercept, 3 is the number of rooms, 1390 is the year it was built, and 80 indicates the size.

original Formula is: $  \widehat{y} = w_{0} + w_{1}x$  but in D dimension will be like so:

% % Matrix with name, line, and text below the line
% $$\text{Let } x^{(1)} = \underline{ \begin{pmatrix}  1 \\  3 \\  1390 \\  80 \end{pmatrix}}  \quad \raisebox{-10ex}{\hspace{-2cm} \text{Explanation}} % Move left by 2cm$$


$$
\underbrace{ 
\begin{bmatrix}
 1 & 3 & 1390 & 80 \\
 \vdots & \vdots & \vdots & \vdots \\
 1 & 2 & 1399 & 160
\end{bmatrix} 
}_{X (data of n houses), each row is features from 0 to D} 
\times \underbrace{ 
\begin{bmatrix}
w_{0}  \\
w_{1}  \\
\vdots \\
w_{D} 
\end{bmatrix} 
}_{text{w}} =
\underbrace{ 
\begin{bmatrix}
y_{0}  \\
y_{1}  \\
\vdots \\
y_{D}
\end{bmatrix} 
}_{y}  
$$
% % Matrix with a curved line and explanation
% $$\text{Let } x^{(1)} =  
% \begin{tikzpicture}[baseline=(current bounding box.south)]
% \node (m) at (0,0) {
% \begin{pmatrix} 
% 1 \\   3 \\     1390 \\     80   
% \end{pmatrix}   };  
% \draw[thick] (m.south west)..controls +(down:5mm) and +(down:5mm) .. (m.south east);
% \end{tikzpicture}
% \quad \raisebox{-1.5ex}{\hspace{-1.5cm} \text{Explanation}}
% $$

\end{cheatformula}

\pagebreak
\section{Perceptron}
A perceptron is a type of artificial neuron that serves as a fundamental building block for more complex neural networks. It is primarily used for binary classification tasks in linear classification problems. Here’s a breakdown of how a perceptron works in the context of linear classification:

\begin{cheatformula}[1. Structure of a Perceptron]
A perceptron consists of:
- **Inputs**: Features of the data (e.g., $x_1, x_2, \ldots, x_n$).
- **Weights**: Each input is associated with a weight ($w_1, w_2, \ldots, w_n$).
- **Bias**: A bias term ($b$) that allows the model to fit the data better.
- **Activation Function**: A function that determines the output of the perceptron based on the weighted sum of the inputs.
\end{cheatformula}

\begin{cheatformula}[2. Mathematical Representation]
The output of a perceptron can be represented mathematically as follows:

1. **Weighted Sum**:
   $$
   z = w_1x_1 + w_2x_2 + \ldots + w_nx_n + b
   $$
   where $z$ is the weighted sum of the inputs.

2. **Activation Function**:
   The perceptron uses a step function (or Heaviside function) as the activation function:
   $$
   output = 
   \begin{cases} 
   1 & if  z \geq 0 \\
   0 & if  z < 0 
   \end{cases}
   $$
   This means that if the weighted sum $z$ is greater than or equal to zero, the perceptron outputs 1 (indicating one class), and if it is less than zero, it outputs 0 (indicating the other class).
\end{cheatformula}


\begin{cheatformula}[3. Training the Perceptron]
The perceptron is trained using a supervised learning algorithm. The training process involves the following steps:

1. **Initialization**: Start with random weights and bias.

2. **Forward Pass**: For each training example, compute the output using the current weights and bias.

3. **Update Weights**: If the output is incorrect (i.e., does not match the true label), update the weights and bias using the following rule:
   $$
   w_i \leftarrow w_i + \eta (y - \hat{y}) x_i
   $$
   $$
   b \leftarrow b + \eta (y - \hat{y})
   $$
   where:
   - $y$ is the true label (0 or 1).
   - $\hat{y}$ is the predicted output (0 or 1).
   - $\eta$ is the learning rate, a small positive value that controls how much the weights are adjusted.

4. **Repeat**: Continue the process for a specified number of epochs or until the weights converge (i.e., the output stabilizes).
\end{cheatformula}

\begin{cheatformula}[4. Linear Classification]
The perceptron is a linear classifier, meaning it attempts to find a linear decision boundary that separates the two classes in the feature space. The decision boundary is defined by the equation:
$$
w_1x_1 + w_2x_2 + \ldots + w_nx_n + b = 0
$$
In a two-dimensional space, this represents a line, while in higher dimensions, it represents a hyperplane.
\end{cheatformula}

\begin{cheatformula}[5. Limitations]

- **Linearly Separable Data**: The perceptron can only classify data that is linearly separable. If the data cannot be separated by a straight line (or hyperplane), the perceptron will not converge to a solution.
- **Single Layer**: A single perceptron cannot solve problems like XOR, which are not linearly separable. However, multiple perceptrons can be combined into multi-layer networks (neural networks) to handle more complex problems.
\end{cheatformula}



\end{multicols*}
\newpage
\end{document}