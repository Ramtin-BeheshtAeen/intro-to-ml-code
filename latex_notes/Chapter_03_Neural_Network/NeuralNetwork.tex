\documentclass{article}
\usepackage[a4paper, top=2cm,right=1cm,left=1cm,bottom=0.5cm]{geometry}
\usepackage{url}
\usepackage{amsmath}
\usepackage{ifthen}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{tikz} 
\usetikzlibrary{positioning}

\usepackage{xcolor}
\usepackage{listings}
\lstset{
  language=Python,
  basicstyle=\ttfamily\footnotesize,
  keywordstyle=\color{blue},
  stringstyle=\color{red},
  commentstyle=\color{gray},
  showstringspaces=false,
  breaklines=true,
  numbers=left,            % ← Line numbers on the left
  numberstyle=\tiny\color{gray}, % Style of line numbers
  numbersep=2pt,        % Space between numbers and code
  inputencoding=utf8,
  extendedchars=true,
  literate={∂}{{\ensuremath{\partial}}}1 {…}{{\ldots}}1
}

\usepackage{multicol}
\usepackage[compact]{titlesec}

\usepackage{mathtools}
\usepackage{siunitx}
\sisetup{inter-unit-product=\ensuremath{{}\cdot{}}, exponent-product = \cdot}
\DeclareSIUnit\bar{bar}

\usepackage[version=4]{mhchem} 

\makeatletter
\newsavebox\myboxA
\newsavebox\myboxB
\newlength\mylenA
\newcommand*\overbar[2][0.75]{%
    \sbox{\myboxA}{$\m@th#2$}%
    \setbox\myboxB\null% Phantom box
    \ht\myboxB=\ht\myboxA%
    \dp\myboxB=\dp\myboxA%
    \wd\myboxB=#1\wd\myboxA% Scale phantom
    \sbox\myboxB{$\m@th\overline{\copy\myboxB}$}%  Overlined phantom
    \setlength\mylenA{\the\wd\myboxA}%   calc width diff
    \addtolength\mylenA{-\the\wd\myboxB}%
    \ifdim\wd\myboxB<\wd\myboxA%
       \rlap{\hskip 0.5\mylenA\usebox\myboxB}{\usebox\myboxA}%
    \else
        \hskip -0.5\mylenA\rlap{\usebox\myboxA}{\hskip 0.5\mylenA\usebox\myboxB}%
    \fi}
\makeatother
\renewcommand{\arraystretch}{1.2}

\makeatletter
\renewcommand*\env@matrix[1][\arraystretch]{%
  \edef\arraystretch{#1}%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{*\c@MaxMatrixCols c}}
\makeatother

\usepackage{esint}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{enumitem}

\usepackage{float}
\usepackage{fancyhdr}
\usepackage{lastpage,refcount, atbegshi}
\usepackage[hidelinks]{hyperref}
\AtBeginShipout{%
  \ifnum\value{page}=\number\numexpr\getpagerefnumber{LastPage}-2\relax
    \phantomsection\label{preLastPage}
  \fi}

% \usepackage{polyglossia}

% \setmainfont{David CLM}
% \setsansfont{Miriam CLM}

% \setdefaultlanguage{english}
% \setotherlanguage{english}  

% \setmathrm{TeX Gyre Schola}

\makeatletter
\def\xpg@set@normalfont#1{%
  \letcs{\rmfamily}{#1@font@rm}%
  \letcs{\sffamily}{#1@font@sf}%
  \letcs{\ttfamily}{#1@font@tt}%
  \def\normalfont{\protect\xpg@select@fontfamily{#1}}%def instead of gdef
  \gdef\reset@font{\protect\normalfont}%
}
% \addto\inlineextras@english{\xpg@set@normalfont{english}}
% \addto\blockextras@english{\xpg@set@normalfont{english}}
\makeatother

\usepackage[style=english]{csquotes}

\usepackage[continuous]{pagenote}
\renewcommand*{\notesname}{NOTE NAME}
\renewcommand{\sectionname}{SECTION NAME}
\renewcommand{\notenuminnotes}[1]{{\textnormal#1. }}
\makepagenote

\renewcommand\text[1]{\textnormal{\textenglish{#1}}}

\setlength{\parindent}{0pt}
\pagestyle{fancy}     % Set the page style to fancy
\fancyhf{}            % Clear all header and footer fields
\renewcommand{\headrulewidth}{0pt} % No line in header
\setlength{\headsep}{0.2cm}% Set the space between header and text

% Define header content
\fancyhead[L]{Page: {\thepage}} % Left header
\fancyhead[R]{\ifthenelse{\value{page}=1}{\today}{} } % Right header shows date on first page

\def\imagewidth{0.9}

\newenvironment{cheatformula}[1][כותרת]{
    \begin{minipage}{\linewidth}
    \textbf{#1}:
}{
    % changing vertical  space between every cheat formula
    \end{minipage}\\[0ex]
}

\newcommand{\cheatimage}[4][\imagewidth]{
    \begin{figure}[H]
        \centering
        \includegraphics[width=#1\linewidth]{#2}
        \caption{#3}
        \label{#4}
    \end{figure}
}

\newcommand*{\NameAndID}{%
    \par\noindent\makebox[2in]{\hrulefill} \hspace{0.5in}\makebox[2.0in]{\hrulefill}%
    \par\noindent\makebox[2in][r]{Ramtin Behesht Aeen}      \hspace{0.5in}\makebox[2.0in][r]{ramtinba145822@gmail.com}%
}%

\title{Summary of }
\author{Auther}

\begin{document}

\setlength{\columnsep}{1cm}
\setlength{\columnseprule}{1pt}

\makeatletter
\begin{center}
    {\NameAndID}\\[2ex]
    {\Large{\textbf{\@title}}}\\[2ex]
\end{center}
\makeatother

\begin{multicols*}{2}
\raggedcolumns


\section{Neural Network Theory}
\begin{cheatformula}[Neural Network Structure]

  \begin{tikzpicture}[
    node distance=1cm,  % set base distance between nodes
    main/.style = {draw, circle},
    xshift=-2cm
  ]
    \node[main] (z) {$z$};
    \node (x) [left=of z, xshift=-0.3cm] {$x$};          % move x more to the left
    \node (b) [above=of x, yshift=3mm] {$b$};            % lift b up a bit from x
    \node[main] (A) [right=of z, xshift=-0.5cm] {$A$};      % A right of z, no extra shift
    \node (y_hat) [right=of A] {$\hat{y} \,\mathrm{(Predicted)} \mid y\, \mathrm{(real\ values)}$};
    \node[below=of y_hat] {$L(\hat{y}, y)$};
  
    \draw[->] (x) -- (z);
    \draw[->] (b) -- (z);
    \draw[->] (z) -- (A);
    \draw[->] (A) -- (y_hat);
  \end{tikzpicture}
  
\end{cheatformula}

\begin{cheatformula}[backward pass for w]

  \begin{tikzpicture}[
    node distance=1cm,  % set base distance between nodes
    main/.style = {draw, circle},
    xshift=-2cm
  ]
    \node[main] (z) {$z$};
    \node (x) [left=of z, xshift=-0.3cm] {$x$};          % move x more to the left
    \node (b) [above=of x, yshift=3mm] {$b$};            % lift b up a bit from x
    \node[main] (A) [right=of z, xshift=-0.5cm] {$A$};      % A right of z, no extra shift
    \node (y_hat) [right=of A] {$\hat{y} \,\mathrm{(Predicted)} \mid y\, \mathrm{(real\ values)}$};
    \node[below=of y_hat] {$L(\hat{y}, y)$};
  
    \draw[->] (x) -- (z);
    \draw[->] (b) -- (z);
    \draw[->] (z) -- (A);
    \draw[->] (A) -- (y_hat);
    % Back ward pass:
    \draw[->,red] (y_hat) to[bend left=20] node[midway, below] { \color{red} $\frac{\partial L} {\partial \hat{y}}$} (A);
    \draw[->,red] (A) to[bend left=20] node[midway, below] { \color{red} $\frac{\partial \hat{y}} {\partial z}$} (z);
    \draw[->,red] (z) to[bend left=20] node[midway, below] { \color{red} $\frac{\partial z} {\partial w}$} (x);
  \end{tikzpicture}
  
\end{cheatformula}

\begin{cheatformula}[backward pass for b]

  \begin{tikzpicture}[
    node distance=1cm,  % set base distance between nodes
    main/.style = {draw, circle},
    xshift=-2cm
  ]
    \node[main] (z) {$z$};
    \node (x) [left=of z, xshift=-0.3cm] {$x$};          % move x more to the left
    \node (b) [above=of x, yshift=3mm] {$b$};            % lift b up a bit from x
    \node[main] (A) [right=of z, xshift=-0.5cm] {$A$};      % A right of z, no extra shift
    \node (y_hat) [right=of A] {$\hat{y} \,\mathrm{(Predicted)} \mid y\, \mathrm{(real\ values)}$};
    \node[below=of y_hat] {$L(\hat{y}, y)$};
  
    \draw[->] (x) -- (z);
    \draw[->] (b) -- (z);
    \draw[->] (z) -- (A);
    \draw[->] (A) -- (y_hat);
    % Back ward pass:
    \draw[->,magenta] (y_hat) to[bend left=20] node[midway, below] { \color{magenta} $\frac{\partial L} {\partial \hat{y}}$} (A);
    \draw[->,magenta] (A) to[bend left=20] node[midway, below] { \color{magenta} $\frac{\partial \hat{y}} {\partial z}$} (z);
    \draw[->,magenta] (z) to[bend left=20] node[midway, below] { \color{magenta} $\frac{\partial z} {\partial b}$} (b);
  \end{tikzpicture}
  
\end{cheatformula}


\begin{cheatformula}[backward pass for w mathematically]
  \begin{equation}
    L =\underbrace{ L(\underbrace{ \hat{y} \underbrace{ (z(Xw+b)) }_{\frac{\partial z} {\partial w} } }_{\frac{\partial \hat{y}} {\partial z}} )}_{\frac{\partial L} {\partial \hat{y}}}
  \end{equation}
  \begin{equation}
    \frac{\partial L} {\partial w}  = \underbrace{\frac{\partial L} {\partial z}}_{ \frac{\partial L} {\partial \hat{y}}  \frac{\partial \hat{y}} {\partial z} } 
    \frac{\partial z} {\partial w}  
  \end{equation}
  as a Summary:

$$
L = \underbrace{L\left(\hat{y}\right)}_{\frac{\partial L}{\partial \hat{y}}} = \underbrace{L\left(\hat{y}\left(z\right)\right)}_{\frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial z}} = \underbrace{L\left(\hat{y}\left(z(XW + b)\right)\right)}_{\frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial z} \cdot \frac{\partial z}{\partial W}}
$$

Or more explicitly for the chain rule:

$$
\frac{\partial L}{\partial W} = \frac{\partial L}{\partial \hat{y}} \times \frac{\partial \hat{y}}{\partial z} \times \frac{\partial z}{\partial W}
$$

\begin{itemize}
  \item $z = XW + b$
  \item $\hat{y} = f(z)$ (activation function, e.g. softmax)
  \item $L = loss(\hat{y}, y)$
\end{itemize}

The derivatives $\frac{\partial L}{\partial \hat{y}}$, $\frac{\partial \hat{y}}{\partial z}$, and $\frac{\partial z}{\partial W}$ represent the gradient chain from output all the way down to weights.


  
\end{cheatformula}


\section{Coding the Neural Network}
\begin{cheatformula}[Neural Network Structure]

  \[
  \frac{\partial L}{\partial w} 
  = \frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial z} \cdot \frac{\partial z}{\partial w}
  \]
  
  where:
  \begin{itemize}
      \item \textbf{Loss derivative}: $\frac{\partial L}{\partial \hat{y}}$
      \item \textbf{Activation derivative}: $\frac{\partial \hat{y}}{\partial z}$
      \item \textbf{Linear model derivative}: $\frac{\partial z}{\partial w}$
  \end{itemize}
\end{cheatformula}

  \begin{cheatformula}[1. Loss derivative $\left(\frac{\partial L}{\partial \hat{y}}\right)$]
  This is computed in:
  
  \begin{lstlisting}
  def backward(self) -> np.ndarray:  # CrossEntropy
      grad = -self.target / self.prediction / self.target.shape[0]
      return grad
  \end{lstlisting}
  
  \begin{itemize}
      \item \texttt{self.prediction} = $\hat{y}$ (output of softmax)
      \item This gives the gradient from the loss with respect to the softmax output $\hat{y}$.
  \end{itemize}
  \end{cheatformula}
  
  \begin{cheatformula}[Activation derivative $\left(\frac{\partial \hat{y}}{\partial z}\right)$]
  If you have a \texttt{Softmax} layer:
  
  \begin{lstlisting}
  def backward(self, up_grad: np.ndarray) -> np.ndarray:  # Softmax
      ...
      down_grad[i] = np.dot(jacobian, up_grad[i])
  \end{lstlisting}
  
  \begin{itemize}
      \item \texttt{up\_grad} is $\frac{\partial L}{\partial \hat{y}}$ from the loss.
      \item The softmax Jacobian gives $\frac{\partial \hat{y}}{\partial z}$.
      \item Output \texttt{down\_grad} is $\frac{\partial L}{\partial z}$.
      \item This matches step 2 of the chain rule.
  \end{itemize}
  \end{cheatformula}
  
  \begin{cheatformula}[Linear model derivative $\left(\frac{\partial z}{\partial w}\right)$]
  In your \texttt{Linear} layer:
  
  \begin{lstlisting}
  def backward(self, up_grad: np.ndarray) -> np.ndarray:  # Linear
      self.dw = np.dot(self.inp.T, up_grad)  # ∂L/∂w
      self.db = np.sum(up_grad, axis=0, keepdims=True)  # ∂L/∂b
      down_grad = np.dot(up_grad, self.w.T)  # ∂L/∂input
      return down_grad
  \end{lstlisting}
  
  \begin{itemize}
      \item \texttt{up\_grad} is $\frac{\partial L}{\partial z}$ from the activation.
      \item Multiplying with \texttt{inp.T} applies $\frac{\partial z}{\partial w}$ to get $\frac{\partial L}{\partial w}$.
      \item This is step 3 of the professor’s chain rule.
  \end{itemize}

  
  \noindent\textbf{Full Chain in Code:}
  \begin{enumerate}
      \item Loss backward $\rightarrow$ \texttt{CrossEntropy.backward()}
      \[
      \frac{\partial L}{\partial \hat{y}}
      \]
      \item Activation backward $\rightarrow$ \texttt{Softmax.backward()}
      \[
      \frac{\partial L}{\partial z} = \frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial z}
      \]
      \item Linear backward $\rightarrow$ \texttt{Linear.backward()}
      \[
      \frac{\partial L}{\partial w} = \frac{\partial L}{\partial z} \cdot \frac{\partial z}{\partial w}
      \]
  \end{enumerate}

  \end{cheatformula}
    
  
  
  


\begin{cheatformula}[Linear ]
Abstract Python Code for Layer:
\begin{lstlisting}
  class Layer:
    def __init__(self):
        self.inp = None
        self.out = None

    def __call__(self, inp: np.ndarray) -> np.ndarray:
        return self.forward(inp)

    def forward(self, inp: np.ndarray) -> np.ndarray:
        raise NotImplementedError

    def backward(self, up_grad: np.ndarray) -> np.ndarray:
        raise NotImplementedError

    def step(self, lr: float) -> None:
        pass
\end{lstlisting}

\begin{itemize}
  \item[$\circ$] Input features:
  \begin{equation}
a  
  \end{equation}

  \item[$\circ$] Features weights:
  \begin{equation}
  a
  \end{equation}

  \item[$\circ$] Bias term:
  \begin{equation}
  a
  \end{equation}

  
  \item[$\circ$] Activation function :
  \begin{equation}
  a
  \end{equation}

  
  \item[$\circ$] Output of the neuron:
  \begin{equation}
  y
  \end{equation}
\end{itemize}

\begin{lstlisting}
  def greet(name):
      print(f"Hello, {name}!")
  
  greet("World")
  \end{lstlisting}

\end{cheatformula}

\begin{cheatformula}[Squared Error(SE)] 
Most Common error function in linear regression is:
\begin{equation}
SE: ( y^{(i)} - h(x^{(i)}, w))^{2}
\end{equation}
\end{cheatformula}



\begin{cheatformula}[Sum of Squared Errors (SSE)]
Cost function should measure all predictions. Thus a choice could be Sum of Squared Error(SE)
\begin{equation}
SSE: \sum_{i=1}^{N}(y^{(i)} - h(x^{(i)}, w) )
\end{equation}
\end{cheatformula}

\begin{cheatformula}[Solve it analytically for one dimension]
Predicted:
\begin{equation}
\widehat{y} = w_{0} + w_{1}x
\end{equation}
SSE or Cost Function:
\begin{equation}
J(w_{0}, w_{1}) := \sum_{i=1}^{N}(y^{(i)} - \widehat{y}^{(i)})^{2} = \sum_{i=1}^{N}(y^{(i)} -  w_{0} + w_{1}x^{(i)})^{2}
\end{equation}
Assumptions:
\begin{equation}
\frac{\partial J}{\partial w_{0}} = 0 \,, \frac{\partial J}{\partial w_{1}} = 0
\end{equation}
$\frac{\partial J}{\partial w_{0}} = 0:$ thus:
\begin{equation}
\frac{\partial }{\partial w_{0}}  ( \sum_{i=1}^{N}(y^{(i)} -  (w_{0} + w_{1}x^{(i)}))^{2}) = 0  \, \footnotemark
\end{equation}

\begin{equation}
-2 \sum_{i=1}^{N}(y^{(i)} -  w_{0} - w_{1}x^{(i)}) = 0
\end{equation}
\footnotetext{\textsuperscript{\thefootnote} $fog(x)' = f(g(x))' g(x)'$}

For this equation to equal zero, the following condition must be met:
\begin{itemize}
\item[$\circ$] $\sum_{i=1}^{N}y^{(i)} = 0 := Y$ 
\item[$\circ$] $\sum_{i=1}^{N}-w_{0}  = 0 := n w_{0} $ 
\item[$\circ$] $\sum_{i=1}^{N}-w_{1}x^{(i)}  = 0 := X $ 
\end{itemize}
Thus:
\begin{equation}
0 = Y - n w_{0} -w_{1}x^{(i)} \longrightarrow  w_{0} = \frac{(Y -w_{1}x^{(i)})}{n}
\end{equation}
\end{cheatformula}


\pagebreak




\end{multicols*}
\newpage

\end{document}