<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Auther" />
  <title>Summary of </title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
</head>
<body>
<header id="title-block-header">
<h1 class="title">Summary of </h1>
<p class="author">Auther</p>
</header>
<div class="center">
<p><br />
<span><span><strong></strong></span></span><br />
</p>
</div>
<div class="multicols*">
<p><span>2</span></p>
<h1 id="neural-network-theory">Neural Network Theory</h1>
<div class="minipage">
<p><strong>Neural Network Structure</strong>:</p>
</div>
<p><br />
</p>
<div class="minipage">
<p><strong>backward pass for w</strong>:</p>
</div>
<p><br />
</p>
<div class="minipage">
<p><strong>backward pass for b</strong>:</p>
</div>
<p><br />
</p>
<div class="minipage">
<p><strong>backward pass for w mathematically</strong>:</p>
<p><span class="math display">\[L =\underbrace{ L(\underbrace{ \hat{y}
\underbrace{ (z(Xw+b)) }_{\frac{\partial z} {\partial w} }
}_{\frac{\partial \hat{y}} {\partial z}} )}_{\frac{\partial L} {\partial
\hat{y}}}\]</span> <span class="math display">\[\frac{\partial L}
{\partial w}  = \underbrace{\frac{\partial L} {\partial z}}_{
\frac{\partial L} {\partial \hat{y}}  \frac{\partial \hat{y}} {\partial
z} }
    \frac{\partial z} {\partial w}\]</span> as a Summary:</p>
<p><span class="math display">\[L =
\underbrace{L\left(\hat{y}\right)}_{\frac{\partial L}{\partial \hat{y}}}
= \underbrace{L\left(\hat{y}\left(z\right)\right)}_{\frac{\partial
L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial z}} =
\underbrace{L\left(\hat{y}\left(z(XW + b)\right)\right)}_{\frac{\partial
L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial z} \cdot
\frac{\partial z}{\partial W}}\]</span></p>
<p>Or more explicitly for the chain rule:</p>
<p><span class="math display">\[\frac{\partial L}{\partial W} =
\frac{\partial L}{\partial \hat{y}} \times \frac{\partial
\hat{y}}{\partial z} \times \frac{\partial z}{\partial W}\]</span></p>
<ul>
<li><p><span class="math inline">\(z = XW + b\)</span></p></li>
<li><p><span class="math inline">\(\hat{y} = f(z)\)</span> (activation
function, e.g. softmax)</p></li>
<li><p><span class="math inline">\(L = loss(\hat{y},
y)\)</span></p></li>
</ul>
<p>The derivatives <span class="math inline">\(\frac{\partial
L}{\partial \hat{y}}\)</span>, <span
class="math inline">\(\frac{\partial \hat{y}}{\partial z}\)</span>, and
<span class="math inline">\(\frac{\partial z}{\partial W}\)</span>
represent the gradient chain from output all the way down to
weights.</p>
</div>
<p><br />
</p>
<h1 id="coding-the-neural-network">Coding the Neural Network</h1>
<div class="minipage">
<p><strong>Neural Network Structure</strong>:</p>
<p><span class="math display">\[\frac{\partial L}{\partial w}
  = \frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial
\hat{y}}{\partial z} \cdot \frac{\partial z}{\partial w}\]</span></p>
<p>where:</p>
<ul>
<li><p><strong>Loss derivative</strong>: <span
class="math inline">\(\frac{\partial L}{\partial
\hat{y}}\)</span></p></li>
<li><p><strong>Activation derivative</strong>: <span
class="math inline">\(\frac{\partial \hat{y}}{\partial
z}\)</span></p></li>
<li><p><strong>Linear model derivative</strong>: <span
class="math inline">\(\frac{\partial z}{\partial w}\)</span></p></li>
</ul>
</div>
<p><br />
</p>
<div class="minipage">
<p><strong>1. Loss derivative <span
class="math inline">\(\left(\frac{\partial L}{\partial
\hat{y}}\right)\)</span></strong>:</p>
<p>This is computed in:</p>
<pre><code>  def backward(self) -&gt; np.ndarray:  # CrossEntropy
      grad = -self.target / self.prediction / self.target.shape[0]
      return grad</code></pre>
<ul>
<li><p><code>self.prediction</code> = <span
class="math inline">\(\hat{y}\)</span> (output of softmax)</p></li>
<li><p>This gives the gradient from the loss with respect to the softmax
output <span class="math inline">\(\hat{y}\)</span>.</p></li>
</ul>
</div>
<p><br />
</p>
<div class="minipage">
<p><strong>Activation derivative <span
class="math inline">\(\left(\frac{\partial \hat{y}}{\partial
z}\right)\)</span></strong>:</p>
<p>If you have a <code>Softmax</code> layer:</p>
<pre><code>  def backward(self, up_grad: np.ndarray) -&gt; np.ndarray:  # Softmax
      ...
      down_grad[i] = np.dot(jacobian, up_grad[i])</code></pre>
<ul>
<li><p><code>up_grad</code> is <span
class="math inline">\(\frac{\partial L}{\partial \hat{y}}\)</span> from
the loss.</p></li>
<li><p>The softmax Jacobian gives <span
class="math inline">\(\frac{\partial \hat{y}}{\partial
z}\)</span>.</p></li>
<li><p>Output <code>down_grad</code> is <span
class="math inline">\(\frac{\partial L}{\partial z}\)</span>.</p></li>
<li><p>This matches step 2 of the chain rule.</p></li>
</ul>
</div>
<p><br />
</p>
<div class="minipage">
<p><strong>Linear model derivative <span
class="math inline">\(\left(\frac{\partial z}{\partial
w}\right)\)</span></strong>:</p>
<p>In your <code>Linear</code> layer:</p>
<pre><code>  def backward(self, up_grad: np.ndarray) -&gt; np.ndarray:  # Linear
      self.dw = np.dot(self.inp.T, up_grad)  # ∂L/∂w
      self.db = np.sum(up_grad, axis=0, keepdims=True)  # ∂L/∂b
      down_grad = np.dot(up_grad, self.w.T)  # ∂L/∂input
      return down_grad</code></pre>
<ul>
<li><p><code>up_grad</code> is <span
class="math inline">\(\frac{\partial L}{\partial z}\)</span> from the
activation.</p></li>
<li><p>Multiplying with <code>inp.T</code> applies <span
class="math inline">\(\frac{\partial z}{\partial w}\)</span> to get
<span class="math inline">\(\frac{\partial L}{\partial
w}\)</span>.</p></li>
<li><p>This is step 3 of the professor’s chain rule.</p></li>
</ul>
<p><strong>Full Chain in Code:</strong></p>
<ol>
<li><p>Loss backward <span class="math inline">\(\rightarrow\)</span>
<code>CrossEntropy.backward()</code> <span
class="math display">\[\frac{\partial L}{\partial
\hat{y}}\]</span></p></li>
<li><p>Activation backward <span
class="math inline">\(\rightarrow\)</span>
<code>Softmax.backward()</code> <span
class="math display">\[\frac{\partial L}{\partial z} = \frac{\partial
L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial
z}\]</span></p></li>
<li><p>Linear backward <span class="math inline">\(\rightarrow\)</span>
<code>Linear.backward()</code> <span
class="math display">\[\frac{\partial L}{\partial w} = \frac{\partial
L}{\partial z} \cdot \frac{\partial z}{\partial w}\]</span></p></li>
</ol>
</div>
<p><br />
</p>
<div class="minipage">
<p><strong>Linear</strong> :</p>
<p>Abstract Python Code for Layer:</p>
<pre><code>  class Layer:
    def __init__(self):
        self.inp = None
        self.out = None

    def __call__(self, inp: np.ndarray) -&gt; np.ndarray:
        return self.forward(inp)

    def forward(self, inp: np.ndarray) -&gt; np.ndarray:
        raise NotImplementedError

    def backward(self, up_grad: np.ndarray) -&gt; np.ndarray:
        raise NotImplementedError

    def step(self, lr: float) -&gt; None:
        pass</code></pre>
<ul>
<li><p>Input features: <span class="math display">\[a\]</span></p></li>
<li><p>Features weights: <span
class="math display">\[a\]</span></p></li>
<li><p>Bias term: <span class="math display">\[a\]</span></p></li>
<li><p>Activation function : <span
class="math display">\[a\]</span></p></li>
<li><p>Output of the neuron: <span
class="math display">\[y\]</span></p></li>
</ul>
<pre><code>  def greet(name):
      print(f&quot;Hello, {name}!&quot;)
  
  greet(&quot;World&quot;)</code></pre>
</div>
<p><br />
</p>
<div class="minipage">
<p><strong>Squared Error(SE)</strong>:</p>
<p>Most Common error function in linear regression is: <span
class="math display">\[SE: ( y^{(i)} - h(x^{(i)}, w))^{2}\]</span></p>
</div>
<p><br />
</p>
<div class="minipage">
<p><strong>Sum of Squared Errors (SSE)</strong>:</p>
<p>Cost function should measure all predictions. Thus a choice could be
Sum of Squared Error(SE) <span class="math display">\[SSE:
\sum_{i=1}^{N}(y^{(i)} - h(x^{(i)}, w) )\]</span></p>
</div>
<p><br />
</p>
<div class="minipage">
<p><strong>Solve it analytically for one dimension</strong>:</p>
<p>Predicted: <span class="math display">\[\widehat{y} = w_{0} +
w_{1}x\]</span> SSE or Cost Function: <span
class="math display">\[J(w_{0}, w_{1}) := \sum_{i=1}^{N}(y^{(i)} -
\widehat{y}^{(i)})^{2} = \sum_{i=1}^{N}(y^{(i)} -  w_{0} +
w_{1}x^{(i)})^{2}\]</span> Assumptions: <span
class="math display">\[\frac{\partial J}{\partial w_{0}} = 0 \,,
\frac{\partial J}{\partial w_{1}} = 0\]</span> <span
class="math inline">\(\frac{\partial J}{\partial w_{0}} = 0:\)</span>
thus: <span class="math display">\[\frac{\partial }{\partial w_{0}}  (
\sum_{i=1}^{N}(y^{(i)} -  (w_{0} + w_{1}x^{(i)}))^{2}) = 0  \,
\footnotemark\]</span></p>
<p><span class="math display">\[-2 \sum_{i=1}^{N}(y^{(i)} -  w_{0} -
w_{1}x^{(i)}) = 0\]</span></p>
<p>For this equation to equal zero, the following condition must be
met:</p>
<ul>
<li><p><span class="math inline">\(\sum_{i=1}^{N}y^{(i)} = 0 :=
Y\)</span></p></li>
<li><p><span class="math inline">\(\sum_{i=1}^{N}-w_{0}  = 0 := n
w_{0}\)</span></p></li>
<li><p><span class="math inline">\(\sum_{i=1}^{N}-w_{1}x^{(i)}  = 0 :=
X\)</span></p></li>
</ul>
<p>Thus: <span class="math display">\[0 = Y - n w_{0} -w_{1}x^{(i)}
\longrightarrow  w_{0} = \frac{(Y -w_{1}x^{(i)})}{n}\]</span></p>
</div>
<p><br />
</p>
</div>
</body>
</html>
