{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import trange\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Neural Network Training Step\n",
    "1. **Forward Pass**\n",
    "* **Input data** $X$ is fed into the network.\n",
    "* Data flows through each **layer**, which usually consists of:\n",
    "\n",
    "  * **Linear/weight layers**: compute $Z = XW + b$\n",
    "  * **Activation layers**: apply non-linear function $A = f(Z)$ (like ReLU, Softmax)\n",
    "* Final layer produces output $\\hat{y}$ (e.g., class probabilities).\n",
    "* Compute the **loss** $L(\\hat{y}, y)$ comparing output to true labels $y$.\n",
    "\n",
    "2. **Backward Pass (Backpropagation)**\n",
    "Goal: calculate gradients of loss w\\.r.t. **all parameters** (weights and biases), so you can update them with gradient descent.\n",
    "\n",
    "2.1. **Compute gradient of loss w\\.r.t. output layer output**\n",
    "\n",
    "* Using loss function derivative, e.g., for cross-entropy loss combined with softmax:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\hat{y}} = \\hat{y} - y\n",
    "$$\n",
    "\n",
    "2.2. **Backprop through output activation (if separate)**\n",
    "\n",
    "* If softmax is separate, calculate gradient of loss w\\.r.t. softmax input $Z$ using Jacobian or simplified form above.\n",
    "* If softmax + cross-entropy are combined, this step is usually optimized.\n",
    "\n",
    "Step 2.3: Backprop through each layer **in reverse order**\n",
    "\n",
    "For each layer going backward:\n",
    "\n",
    "* **Activation layers (like ReLU):**\n",
    "\n",
    "  $$\n",
    "  \\frac{\\partial L}{\\partial Z} = \\frac{\\partial L}{\\partial A} \\odot f'(Z)\n",
    "  $$\n",
    "\n",
    "  * Multiply upstream gradient by derivative of activation (element-wise).\n",
    "\n",
    "* **Weight layers (like Dense/Fully connected):**\n",
    "\n",
    "  $$\n",
    "  \\frac{\\partial L}{\\partial W} = X^T \\cdot \\frac{\\partial L}{\\partial Z}\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  \\frac{\\partial L}{\\partial b} = \\sum \\frac{\\partial L}{\\partial Z}\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  \\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial Z} \\cdot W^T\n",
    "  $$\n",
    "\n",
    "  * Calculate gradient w\\.r.t weights and biases.\n",
    "  * Calculate gradient w\\.r.t input to pass to the previous layer.\n",
    "\n",
    "3. **Update weights**\n",
    "\n",
    "* Use gradients to update weights with gradient descent or a variant, e.g.,\n",
    "\n",
    "$$\n",
    "W := W - \\eta \\frac{\\partial L}{\\partial W}\n",
    "$$\n",
    "\n",
    "where $\\eta$ is the learning rate.\n",
    "---\n",
    "Full Example: Simple 2-layer network (Dense + ReLU + Dense + Softmax)\n",
    "\n",
    "* Forward pass:\n",
    "\n",
    "  ```\n",
    "  X -> Dense1 -> Z1\n",
    "  Z1 -> ReLU -> A1\n",
    "  A1 -> Dense2 -> Z2\n",
    "  Z2 -> Softmax -> output probabilities\n",
    "  ```\n",
    "\n",
    "* Backward pass:\n",
    "\n",
    "  ```\n",
    "  dL/dOutput = (output - true_labels)  # for softmax + cross-entropy\n",
    "  dL/dZ2 = dL/dOutput\n",
    "  dL/dA1 = Dense2.backward(dL/dZ2)\n",
    "  dL/dZ1 = ReLU.backward(dL/dA1)\n",
    "  dL/dX = Dense1.backward(dL/dZ1)\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement the Neural Network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract Layer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.inp  = None \n",
    "        self.out = None\n",
    "\n",
    "    def __call__(self, inp: np.ndarray) -> np.ndarray:\n",
    "        return self.forward(inp)\n",
    "    \n",
    "    def forward(self, inp: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self, up_graad: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def step(self, learning_rate: float) -> None:\n",
    "        pass        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Layers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialization**\n",
    "- `self.w`: Represents the weight matrix of shape `(in_dim, out_dim)`, initialized using small random values.\n",
    "- `self.b`: Bias vector of shape `(1, out_dim)`, initialized to zeros.\n",
    "- `self.dw` and `self.db`: These store the computed gradients of weights and biases during backpropagation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Forward Pass**\n",
    "- The forward pass computes:\n",
    "$$\\mathbf{out} = \\mathbf{inp} \\cdot \\mathbf{W} + \\mathbf{b}$$\n",
    "where:\n",
    "  - `inp`: inp matrix of shape `(batch_size, in_dim)`\n",
    "  - `self.w`: Weight matrix of shape `(in_dim, out_dim)`\n",
    "  -\t`self.b`: Bias matrix of shape `(1, out_dim)`\n",
    "-\tThe result is a matrix out of shape `(batch_size, out_dim)`.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## **Backward Pass**\n",
    "- The backward pass computes gradients needed for updating the weights and biases. Given the upstream gradient `up_grad` (from the loss with respect to the output of this layer), we calculate:\n",
    "  - Gradient w.r.t. weights (`self.dw`):\n",
    "    $ \\frac{\\partial L}{\\partial W} = \\mathbf{inp}^T \\cdot \\text{up\\_grad} $\n",
    "  - Gradient w.r.t. biases (`self.db`):\n",
    "    $ \\frac{\\partial L}{\\partial b} = \\sum \\text{up\\_grad} \\text{ (summed across batch)} $\n",
    "  - Gradient to propagate to the previous layer (`down_grad`):\n",
    "    $ \\text{down\\_grad} = \\text{up\\_grad} \\cdot W^T $\n",
    "- This allows the gradient to flow backward to earlier layers.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Step Method**\n",
    "- Updates the weights and biases using the computed gradients and learning rate (`learning_rate`):\n",
    "    $$W = W - lr \\cdot \\frac{\\partial L}{\\partial W}$$\n",
    "    $$b = b - lr \\cdot \\frac{\\partial L}{\\partial b}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self, in_dim: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        # HE initialization:\n",
    "        self.w  = 0.1 * np.random.randn(in_dim, out_dim)\n",
    "        #self.w = np.random.randn(in_dim, out_dim) * np.sqrt(2. / in_dim)  # He initialization\n",
    "        self.b  = np.zeros((1, out_dim))\n",
    "        self.dw = np.zeros_like(self.w)\n",
    "        self.db = np.zeros_like(self.b) \n",
    "    \n",
    "    def forward(self, inp: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Perform the linear transformation: output = inp * W + b\"\"\"\n",
    "        self.inp = inp\n",
    "        self.out = np.dot(inp, self.w) + self.b\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self, up_grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Backpropagate the gradients through this layer: \"\"\"\n",
    "        # dw = round L / round w \n",
    "        self.dw = np.dot(self.inp.T, up_grad)\n",
    "        # dw = round L / round b\n",
    "        self.db = np.sum(up_grad, axis=0, keepdims=True)\n",
    "\n",
    "        down_grad = np.dot(up_grad, self.w.T)\n",
    "        return down_grad\n",
    "    \n",
    "    def step(self, learning_rate: float) -> None:\n",
    "        self.w -= learning_rate * self.dw\n",
    "        self.b -= learning_rate * self.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    def forward(self, inp: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"ReLU Activation function: f(x) = max(0, x)\"\"\"\n",
    "        self.inp = inp\n",
    "        self.out = np.maximum(0, inp)\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self, up_grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for ReLU: derivative is 1 where input > 0, else 0.\"\"\"\n",
    "        down_grad = up_grad * (self.inp > 0)  # Efficient boolean indexing\n",
    "        return down_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Layer):\n",
    "    def forward(self, inp: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Softmax activation: f(x) = exp(x) / sum(esp(x))\"\"\"\n",
    "        # subtract max for numerical stability:\n",
    "        exp_values = np.exp(inp - np.max(inp, axis=1, keepdims=True) )\n",
    "        self.out   = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, up_grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for Softmax using the Jacobian matrix.\"\"\"\n",
    "        down_grad = np.empty_like(up_grad)\n",
    "        for i in range(up_grad.shape[0]):\n",
    "            single_output = self.out[i].reshape(-1, 1)\n",
    "            jacobian      = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            down_grad[i]  = np.dot(jacobian, up_grad[i])\n",
    "        return down_grad\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why does softmax use the Jacobian as a derivative:\n",
    "A Jacobian is used when the output of a function depends on all inputs.\n",
    "Thanks for pointing that out — let's slow it down and **explain what it means for a function to be element-wise**, and why that matters for backpropagation.\n",
    "\n",
    "---\n",
    "\n",
    "What does *element-wise* mean?\n",
    "\n",
    "When we say a function is **element-wise**, we mean that:\n",
    "\n",
    "> **Each output value depends on exactly one input value**, and not on the others.\n",
    "\n",
    "In symbols:\n",
    "* Input: $x = [x_1, x_2, x_3]$\n",
    "* Output: $y = f(x) = [f(x_1), f(x_2), f(x_3)]$\n",
    "Each $y_i = f(x_i)$, completely **independent** of the other $x_j$\n",
    "---\n",
    "Example: ReLU is element-wise\n",
    "\n",
    "```python\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "```\n",
    "Input:\n",
    "\n",
    "```python\n",
    "x = [-2, 3, -1]\n",
    "```\n",
    "Output:\n",
    "\n",
    "```python\n",
    "relu(x) = [0, 3, 0]\n",
    "```\n",
    "\n",
    "* The output at position 0 (which is 0) comes **only** from the input at position 0 (which is -2).\n",
    "* It does **not** depend on `x[1]` or `x[2]`.\n",
    "\n",
    "So we can compute the derivative (gradient) for each position **individually**:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_i}{\\partial x_i} = \n",
    "\\begin{cases}\n",
    "1 & \\text{if } x_i > 0 \\\\\n",
    "0 & \\text{if } x_i \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "And this is why the backward pass is so simple:\n",
    "\n",
    "```python\n",
    "down_grad = up_grad * (self.inp > 0)\n",
    "```\n",
    "\n",
    "Just multiply the gradient at each index by either 1 or 0 — no interference between indices.\n",
    "\n",
    "---\n",
    "\n",
    "Softmax is **not** element-wise:\n",
    "$$\n",
    "\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}\n",
    "$$\n",
    "That means each output $y_i$ is:\n",
    "* a fraction that includes **every** $z_j$ in the denominator.\n",
    "So the output $y_0$ depends on:\n",
    "* $z_0$, because of the numerator,\n",
    "* **and** all $z_j$ (including $z_1, z_2, ...$) because they’re in the denominator.\n",
    "Same goes for $y_1, y_2$, etc.\n",
    "\n",
    "Result:\n",
    "\n",
    "* Changing one input $z_j$ affects **all** outputs $y_i$\n",
    "* So we can't compute the derivative of $y_i$ w\\.r.t. $z_j$ independently\n",
    "* That’s why we need the **Jacobian matrix**, to keep track of **how every input affects every output**\n",
    "\n",
    "\n",
    "\n",
    "| Layer   | Formula                                   | Output depends on…    | Needs Jacobian? |\n",
    "| ------- | ----------------------------------------- | --------------------- | --------------- |\n",
    "| ReLU    | $f(x_i) = \\max(0, x_i)$                   | Only one input $x_i$  | ❌ No            |\n",
    "| Softmax | $f(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$ | All inputs $x_0..x_n$ | ✅ Yes           |\n",
    "\n",
    "---\n",
    "\n",
    "In backpropagation:\n",
    "\n",
    "* For **ReLU**, it's enough to use a simple mask.\n",
    "* For **Softmax**, you must account for all interactions — which is what the Jacobian matrix does.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstrac Loss Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def __init__(self):\n",
    "        self.predicted = None\n",
    "        self.target    = None\n",
    "        self.loss      = None\n",
    "    \n",
    "    def __call__(self, prediction: np.ndarray, target: np.ndarray) -> float:\n",
    "        return self.forward(prediction, target)\n",
    "    \n",
    "    def forward(self, prediction:np.ndarray, target: np.ndarray) -> float:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self) -> np.ndarray:\n",
    "        raise NotImplementedError        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Entropy Loss\n",
    "\n",
    "Cross-entropy loss is typically used in classification tasks since it measures the dissimilarity between the true distribution (target) and the predicted probability distribution (prediction):\n",
    "\n",
    "$$L = - \\frac{1}{N} \\sum_{i} \\sum_{c} y_{ic} \\log(p_{ic})$$\n",
    "\n",
    "where $y_{ic}$ is the one-hot encoded true label (target), $p_{ic}$ is the predicted probability (output from Softmax) and $N$ is the batch size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy(Loss):\n",
    "    def forward(self, prediction: np.ndarray, target: np.ndarray) -> float:\n",
    "        \"\"\"Cross-Entropy Loss for classification.\"\"\"\n",
    "        self.prediction = prediction\n",
    "        self.target = target\n",
    "        # Clip predictions to avoid log(0)\n",
    "        clipped_pred = np.clip(prediction, 1e-12, 1.0)\n",
    "        # Compute and return the loss\n",
    "        self.loss = -np.mean(np.sum(target * np.log(clipped_pred), axis=1))\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self) -> np.ndarray:\n",
    "        \"\"\"Gradient of Cross-Entropy Loss.\"\"\"\n",
    "        # Gradient wrt prediction (assuming softmax and one-hot targets)\n",
    "        grad = -self.target / self.prediction / self.target.shape[0]\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, layers: list[Layer], loss_func: Loss, learning_rate: float) -> None:\n",
    "        self.layers        = layers\n",
    "        self.loss_func     = loss_func\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def __call__(self, input: np.ndarray) -> np.ndarray:\n",
    "        return self.forward(input)\n",
    "    \n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" pass input through each layer sequentially \"\"\"\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        return input\n",
    "    \n",
    "    def loss(self, prediction: np.ndarray, target: np.ndarray) -> float:\n",
    "        \"\"\"Calculate Loss\"\"\"\n",
    "        return self.loss_func(prediction, target)\n",
    "    \n",
    "    def backward(self) -> None:\n",
    "        up_grad = self.loss_func.backward()\n",
    "        for layer in reversed(self.layers):\n",
    "            up_grad = layer.backward(up_grad)\n",
    "    \n",
    "    def update(self) -> None:\n",
    "        for layer in self.layers:\n",
    "            layer.step(self.learning_rate)\n",
    "    \n",
    "    def train(self, x_train: np.ndarray, y_train: np.ndarray, epochs: int, batch_size: int) -> np.ndarray:\n",
    "        losses = np.empty(epochs)\n",
    "        for epoch in (pbar := trange(epochs)):\n",
    "            running_loss = 0.0\n",
    "            for i in range(0, len(x_train), batch_size):\n",
    "                x_batch = x_train[i:i + batch_size]\n",
    "                y_batch = y_train[i:i + batch_size]\n",
    "\n",
    "                #Forward pass:\n",
    "                prediction = self.forward(x_batch)\n",
    "\n",
    "                #Compute loss:\n",
    "                running_loss += self.loss(prediction, y_batch) * batch_size\n",
    "\n",
    "                #Backward pass:\n",
    "                self.backward()\n",
    "\n",
    "                #Update the parameters:\n",
    "                self.update()\n",
    "            \n",
    "            #Normalize running loss by total number of samples:\n",
    "            running_loss /= len(x_train)\n",
    "            pbar.set_description(f\"Loss: {running_loss:.3f}\")\n",
    "            losses[epoch] = running_loss\n",
    "\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use MLP to solve a problem: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating the Fashion-MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = fetch_openml(\"Fashion-MNIST\", parser='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.utils._bunch.Bunch'>\n",
      "type of the fashion_mnist: <class 'sklearn.utils._bunch.Bunch'> \n",
      "\n",
      "keys of the fashion_mnist: dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])\n"
     ]
    }
   ],
   "source": [
    "print(type(fashion_mnist))\n",
    "print(f\"type of the fashion_mnist: {type(fashion_mnist)} \\n\")\n",
    "print(f\"keys of the fashion_mnist: {fashion_mnist.keys()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(fashion_mnist['data']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the dataset: (70000, 784)\n",
      "data of the fashion_mnist:    pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  pixel9  \\\n",
      "0       0       0       0       0       0       0       0       0       0   \n",
      "1       0       0       0       0       0       1       0       0       0   \n",
      "2       0       0       0       0       0       0       0       0       0   \n",
      "3       0       0       0       0       0       0       0       0      33   \n",
      "4       0       0       0       0       0       0       0       0       0   \n",
      "\n",
      "   pixel10  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
      "0        0  ...         0         0         0         0         0         0   \n",
      "1        0  ...       119       114       130        76         0         0   \n",
      "2       22  ...         0         0         1         0         0         0   \n",
      "3       96  ...         0         0         0         0         0         0   \n",
      "4        0  ...         0         0         0         0         0         0   \n",
      "\n",
      "   pixel781  pixel782  pixel783  pixel784  \n",
      "0         0         0         0         0  \n",
      "1         0         0         0         0  \n",
      "2         0         0         0         0  \n",
      "3         0         0         0         0  \n",
      "4         0         0         0         0  \n",
      "\n",
      "[5 rows x 784 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"shape of the dataset: {fashion_mnist['data'].shape}\")\n",
    "print(f\"data of the fashion_mnist: {fashion_mnist['data'].head()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing explained:\n",
    "- fashion_mnist['data'].shape[0] = 70000 → number of images (indexes from 0 to 69999).\n",
    "- fashion_mnist['data'].shape[1] = 784 → number of pixels in each image (indexes from 0 to 783)\n",
    "\n",
    "#### So:\n",
    "- When you do fashion_mnist['data'][index] — you are selecting the entire image at that row index (a 1D array of 784 pixels).\n",
    "\n",
    "- When you do fashion_mnist['data'][index][pixel] — you are selecting a single pixel value from the image at index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel1      0\n",
      "pixel2      0\n",
      "pixel3      0\n",
      "pixel4      0\n",
      "pixel5      0\n",
      "           ..\n",
      "pixel780    0\n",
      "pixel781    0\n",
      "pixel782    0\n",
      "pixel783    0\n",
      "pixel784    0\n",
      "Name: 0, Length: 784, dtype: int64\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Access first row (image 0) in the DataFrame\n",
    "first_image_row = fashion_mnist['data'].iloc[0]  # iloc for positional indexing\n",
    "\n",
    "print(first_image_row)  # This is a pandas Series with pixel columns\n",
    "\n",
    "# Access pixel 0 value in this row:\n",
    "print(first_image_row.iloc[0])\n",
    "\n",
    "# Or simply chain:\n",
    "print(fashion_mnist['data'].iloc[0].iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature names of the fashion_mnist: ['pixel1', 'pixel2', 'pixel3', 'pixel4', 'pixel5', 'pixel6', 'pixel7', 'pixel8', 'pixel9', 'pixel10', 'pixel11', 'pixel12', 'pixel13', 'pixel14', 'pixel15', 'pixel16', 'pixel17', 'pixel18', 'pixel19', 'pixel20', 'pixel21', 'pixel22', 'pixel23', 'pixel24', 'pixel25', 'pixel26', 'pixel27', 'pixel28', 'pixel29', 'pixel30', 'pixel31', 'pixel32', 'pixel33', 'pixel34', 'pixel35', 'pixel36', 'pixel37', 'pixel38', 'pixel39', 'pixel40', 'pixel41', 'pixel42', 'pixel43', 'pixel44', 'pixel45', 'pixel46', 'pixel47', 'pixel48', 'pixel49', 'pixel50', 'pixel51', 'pixel52', 'pixel53', 'pixel54', 'pixel55', 'pixel56', 'pixel57', 'pixel58', 'pixel59', 'pixel60', 'pixel61', 'pixel62', 'pixel63', 'pixel64', 'pixel65', 'pixel66', 'pixel67', 'pixel68', 'pixel69', 'pixel70', 'pixel71', 'pixel72', 'pixel73', 'pixel74', 'pixel75', 'pixel76', 'pixel77', 'pixel78', 'pixel79', 'pixel80', 'pixel81', 'pixel82', 'pixel83', 'pixel84', 'pixel85', 'pixel86', 'pixel87', 'pixel88', 'pixel89', 'pixel90', 'pixel91', 'pixel92', 'pixel93', 'pixel94', 'pixel95', 'pixel96', 'pixel97', 'pixel98', 'pixel99', 'pixel100', 'pixel101', 'pixel102', 'pixel103', 'pixel104', 'pixel105', 'pixel106', 'pixel107', 'pixel108', 'pixel109', 'pixel110', 'pixel111', 'pixel112', 'pixel113', 'pixel114', 'pixel115', 'pixel116', 'pixel117', 'pixel118', 'pixel119', 'pixel120', 'pixel121', 'pixel122', 'pixel123', 'pixel124', 'pixel125', 'pixel126', 'pixel127', 'pixel128', 'pixel129', 'pixel130', 'pixel131', 'pixel132', 'pixel133', 'pixel134', 'pixel135', 'pixel136', 'pixel137', 'pixel138', 'pixel139', 'pixel140', 'pixel141', 'pixel142', 'pixel143', 'pixel144', 'pixel145', 'pixel146', 'pixel147', 'pixel148', 'pixel149', 'pixel150', 'pixel151', 'pixel152', 'pixel153', 'pixel154', 'pixel155', 'pixel156', 'pixel157', 'pixel158', 'pixel159', 'pixel160', 'pixel161', 'pixel162', 'pixel163', 'pixel164', 'pixel165', 'pixel166', 'pixel167', 'pixel168', 'pixel169', 'pixel170', 'pixel171', 'pixel172', 'pixel173', 'pixel174', 'pixel175', 'pixel176', 'pixel177', 'pixel178', 'pixel179', 'pixel180', 'pixel181', 'pixel182', 'pixel183', 'pixel184', 'pixel185', 'pixel186', 'pixel187', 'pixel188', 'pixel189', 'pixel190', 'pixel191', 'pixel192', 'pixel193', 'pixel194', 'pixel195', 'pixel196', 'pixel197', 'pixel198', 'pixel199', 'pixel200', 'pixel201', 'pixel202', 'pixel203', 'pixel204', 'pixel205', 'pixel206', 'pixel207', 'pixel208', 'pixel209', 'pixel210', 'pixel211', 'pixel212', 'pixel213', 'pixel214', 'pixel215', 'pixel216', 'pixel217', 'pixel218', 'pixel219', 'pixel220', 'pixel221', 'pixel222', 'pixel223', 'pixel224', 'pixel225', 'pixel226', 'pixel227', 'pixel228', 'pixel229', 'pixel230', 'pixel231', 'pixel232', 'pixel233', 'pixel234', 'pixel235', 'pixel236', 'pixel237', 'pixel238', 'pixel239', 'pixel240', 'pixel241', 'pixel242', 'pixel243', 'pixel244', 'pixel245', 'pixel246', 'pixel247', 'pixel248', 'pixel249', 'pixel250', 'pixel251', 'pixel252', 'pixel253', 'pixel254', 'pixel255', 'pixel256', 'pixel257', 'pixel258', 'pixel259', 'pixel260', 'pixel261', 'pixel262', 'pixel263', 'pixel264', 'pixel265', 'pixel266', 'pixel267', 'pixel268', 'pixel269', 'pixel270', 'pixel271', 'pixel272', 'pixel273', 'pixel274', 'pixel275', 'pixel276', 'pixel277', 'pixel278', 'pixel279', 'pixel280', 'pixel281', 'pixel282', 'pixel283', 'pixel284', 'pixel285', 'pixel286', 'pixel287', 'pixel288', 'pixel289', 'pixel290', 'pixel291', 'pixel292', 'pixel293', 'pixel294', 'pixel295', 'pixel296', 'pixel297', 'pixel298', 'pixel299', 'pixel300', 'pixel301', 'pixel302', 'pixel303', 'pixel304', 'pixel305', 'pixel306', 'pixel307', 'pixel308', 'pixel309', 'pixel310', 'pixel311', 'pixel312', 'pixel313', 'pixel314', 'pixel315', 'pixel316', 'pixel317', 'pixel318', 'pixel319', 'pixel320', 'pixel321', 'pixel322', 'pixel323', 'pixel324', 'pixel325', 'pixel326', 'pixel327', 'pixel328', 'pixel329', 'pixel330', 'pixel331', 'pixel332', 'pixel333', 'pixel334', 'pixel335', 'pixel336', 'pixel337', 'pixel338', 'pixel339', 'pixel340', 'pixel341', 'pixel342', 'pixel343', 'pixel344', 'pixel345', 'pixel346', 'pixel347', 'pixel348', 'pixel349', 'pixel350', 'pixel351', 'pixel352', 'pixel353', 'pixel354', 'pixel355', 'pixel356', 'pixel357', 'pixel358', 'pixel359', 'pixel360', 'pixel361', 'pixel362', 'pixel363', 'pixel364', 'pixel365', 'pixel366', 'pixel367', 'pixel368', 'pixel369', 'pixel370', 'pixel371', 'pixel372', 'pixel373', 'pixel374', 'pixel375', 'pixel376', 'pixel377', 'pixel378', 'pixel379', 'pixel380', 'pixel381', 'pixel382', 'pixel383', 'pixel384', 'pixel385', 'pixel386', 'pixel387', 'pixel388', 'pixel389', 'pixel390', 'pixel391', 'pixel392', 'pixel393', 'pixel394', 'pixel395', 'pixel396', 'pixel397', 'pixel398', 'pixel399', 'pixel400', 'pixel401', 'pixel402', 'pixel403', 'pixel404', 'pixel405', 'pixel406', 'pixel407', 'pixel408', 'pixel409', 'pixel410', 'pixel411', 'pixel412', 'pixel413', 'pixel414', 'pixel415', 'pixel416', 'pixel417', 'pixel418', 'pixel419', 'pixel420', 'pixel421', 'pixel422', 'pixel423', 'pixel424', 'pixel425', 'pixel426', 'pixel427', 'pixel428', 'pixel429', 'pixel430', 'pixel431', 'pixel432', 'pixel433', 'pixel434', 'pixel435', 'pixel436', 'pixel437', 'pixel438', 'pixel439', 'pixel440', 'pixel441', 'pixel442', 'pixel443', 'pixel444', 'pixel445', 'pixel446', 'pixel447', 'pixel448', 'pixel449', 'pixel450', 'pixel451', 'pixel452', 'pixel453', 'pixel454', 'pixel455', 'pixel456', 'pixel457', 'pixel458', 'pixel459', 'pixel460', 'pixel461', 'pixel462', 'pixel463', 'pixel464', 'pixel465', 'pixel466', 'pixel467', 'pixel468', 'pixel469', 'pixel470', 'pixel471', 'pixel472', 'pixel473', 'pixel474', 'pixel475', 'pixel476', 'pixel477', 'pixel478', 'pixel479', 'pixel480', 'pixel481', 'pixel482', 'pixel483', 'pixel484', 'pixel485', 'pixel486', 'pixel487', 'pixel488', 'pixel489', 'pixel490', 'pixel491', 'pixel492', 'pixel493', 'pixel494', 'pixel495', 'pixel496', 'pixel497', 'pixel498', 'pixel499', 'pixel500', 'pixel501', 'pixel502', 'pixel503', 'pixel504', 'pixel505', 'pixel506', 'pixel507', 'pixel508', 'pixel509', 'pixel510', 'pixel511', 'pixel512', 'pixel513', 'pixel514', 'pixel515', 'pixel516', 'pixel517', 'pixel518', 'pixel519', 'pixel520', 'pixel521', 'pixel522', 'pixel523', 'pixel524', 'pixel525', 'pixel526', 'pixel527', 'pixel528', 'pixel529', 'pixel530', 'pixel531', 'pixel532', 'pixel533', 'pixel534', 'pixel535', 'pixel536', 'pixel537', 'pixel538', 'pixel539', 'pixel540', 'pixel541', 'pixel542', 'pixel543', 'pixel544', 'pixel545', 'pixel546', 'pixel547', 'pixel548', 'pixel549', 'pixel550', 'pixel551', 'pixel552', 'pixel553', 'pixel554', 'pixel555', 'pixel556', 'pixel557', 'pixel558', 'pixel559', 'pixel560', 'pixel561', 'pixel562', 'pixel563', 'pixel564', 'pixel565', 'pixel566', 'pixel567', 'pixel568', 'pixel569', 'pixel570', 'pixel571', 'pixel572', 'pixel573', 'pixel574', 'pixel575', 'pixel576', 'pixel577', 'pixel578', 'pixel579', 'pixel580', 'pixel581', 'pixel582', 'pixel583', 'pixel584', 'pixel585', 'pixel586', 'pixel587', 'pixel588', 'pixel589', 'pixel590', 'pixel591', 'pixel592', 'pixel593', 'pixel594', 'pixel595', 'pixel596', 'pixel597', 'pixel598', 'pixel599', 'pixel600', 'pixel601', 'pixel602', 'pixel603', 'pixel604', 'pixel605', 'pixel606', 'pixel607', 'pixel608', 'pixel609', 'pixel610', 'pixel611', 'pixel612', 'pixel613', 'pixel614', 'pixel615', 'pixel616', 'pixel617', 'pixel618', 'pixel619', 'pixel620', 'pixel621', 'pixel622', 'pixel623', 'pixel624', 'pixel625', 'pixel626', 'pixel627', 'pixel628', 'pixel629', 'pixel630', 'pixel631', 'pixel632', 'pixel633', 'pixel634', 'pixel635', 'pixel636', 'pixel637', 'pixel638', 'pixel639', 'pixel640', 'pixel641', 'pixel642', 'pixel643', 'pixel644', 'pixel645', 'pixel646', 'pixel647', 'pixel648', 'pixel649', 'pixel650', 'pixel651', 'pixel652', 'pixel653', 'pixel654', 'pixel655', 'pixel656', 'pixel657', 'pixel658', 'pixel659', 'pixel660', 'pixel661', 'pixel662', 'pixel663', 'pixel664', 'pixel665', 'pixel666', 'pixel667', 'pixel668', 'pixel669', 'pixel670', 'pixel671', 'pixel672', 'pixel673', 'pixel674', 'pixel675', 'pixel676', 'pixel677', 'pixel678', 'pixel679', 'pixel680', 'pixel681', 'pixel682', 'pixel683', 'pixel684', 'pixel685', 'pixel686', 'pixel687', 'pixel688', 'pixel689', 'pixel690', 'pixel691', 'pixel692', 'pixel693', 'pixel694', 'pixel695', 'pixel696', 'pixel697', 'pixel698', 'pixel699', 'pixel700', 'pixel701', 'pixel702', 'pixel703', 'pixel704', 'pixel705', 'pixel706', 'pixel707', 'pixel708', 'pixel709', 'pixel710', 'pixel711', 'pixel712', 'pixel713', 'pixel714', 'pixel715', 'pixel716', 'pixel717', 'pixel718', 'pixel719', 'pixel720', 'pixel721', 'pixel722', 'pixel723', 'pixel724', 'pixel725', 'pixel726', 'pixel727', 'pixel728', 'pixel729', 'pixel730', 'pixel731', 'pixel732', 'pixel733', 'pixel734', 'pixel735', 'pixel736', 'pixel737', 'pixel738', 'pixel739', 'pixel740', 'pixel741', 'pixel742', 'pixel743', 'pixel744', 'pixel745', 'pixel746', 'pixel747', 'pixel748', 'pixel749', 'pixel750', 'pixel751', 'pixel752', 'pixel753', 'pixel754', 'pixel755', 'pixel756', 'pixel757', 'pixel758', 'pixel759', 'pixel760', 'pixel761', 'pixel762', 'pixel763', 'pixel764', 'pixel765', 'pixel766', 'pixel767', 'pixel768', 'pixel769', 'pixel770', 'pixel771', 'pixel772', 'pixel773', 'pixel774', 'pixel775', 'pixel776', 'pixel777', 'pixel778', 'pixel779', 'pixel780', 'pixel781', 'pixel782', 'pixel783', 'pixel784']\n",
      "target names of the fashion_mnist: ['class']\n"
     ]
    }
   ],
   "source": [
    "print(f\"feature names of the fashion_mnist: {fashion_mnist['feature_names']}\")\n",
    "print(f\"target names of the fashion_mnist: {fashion_mnist['target_names']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "1\n",
      "i: class\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "#The dataset’s target_names list only contains the name of the target column, which is \"class\".\n",
    "target_names = fashion_mnist['target_names']\n",
    "\n",
    "print(type(target_names))\n",
    "\n",
    "print(len(target_names))\n",
    "\n",
    "for i in target_names:\n",
    "    print(f\"i: {i}\")\n",
    "    print(type(i))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLot the MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First image pixel array shape: (784,)\n",
      "First pixel value of first image: 0\n",
      "Label number: 9\n",
      "Label name: Ankle boot\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Suppose fashion_mnist is loaded as a dictionary with pandas DataFrames/Series\n",
    "# Example class names mapping:\n",
    "class_names = {\n",
    "    0: \"T-shirt/top\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle boot\"\n",
    "}\n",
    "\n",
    "index = 0  # Index of the image to examine\n",
    "\n",
    "# Access image pixels as pandas Series and convert to numpy array\n",
    "image_pixels = fashion_mnist['data'].iloc[index].values\n",
    "\n",
    "# Access first pixel value\n",
    "first_pixel_value = image_pixels[0]\n",
    "\n",
    "# Access label\n",
    "label = fashion_mnist['target'].iloc[index]\n",
    "\n",
    "print(f\"First image pixel array shape: {image_pixels.shape}\")  # Should be (784,)\n",
    "print(f\"First pixel value of first image: {first_pixel_value}\")\n",
    "print(f\"Label number: {label}\")\n",
    "print(f\"Label name: {class_names[int(label)]}\")\n",
    "\n",
    "# Reshape to 28x28 for visualization\n",
    "image_2d = image_pixels.reshape(28, 28)\n",
    "\n",
    "# Plot the image\n",
    "plt.imshow(image_2d, cmap='gray')\n",
    "plt.title(f\"Class: {class_names[int(label)]}\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "summary of the **Fashion MNIST dataset** :\n",
    "\n",
    "**Fashion MNIST Dataset – Full Summary**\n",
    "\n",
    "* **What it is**:\n",
    "  A benchmark dataset for machine learning, used to train and evaluate image classification models on clothing item recognition.\n",
    "\n",
    "* **Total Samples**: **70,000 grayscale images**\n",
    "\n",
    "  * **60,000 training samples**\n",
    "  * **10,000 test samples**\n",
    "\n",
    "* **Image Details**:\n",
    "\n",
    "  * Each image is **28 × 28 pixels** = **784 pixels total**\n",
    "  * Stored as a **flattened 1D array of 784 values**\n",
    "  * Each pixel is a **grayscale intensity**: value between **0 (black)** and **255 (white)**\n",
    "\n",
    "* **Features**:\n",
    "\n",
    "  * Named `pixel1`, `pixel2`, ..., `pixel784`\n",
    "  * Represent the brightness level of each pixel\n",
    "  * No color (only black and white shades)\n",
    "\n",
    "* **Target Labels**:\n",
    "  One label per image, from **0 to 9**, representing the clothing type\n",
    "\n",
    "| Label | Clothing Item |\n",
    "| ----- | ------------- |\n",
    "| 0     | T-shirt/top   |\n",
    "| 1     | Trouser       |\n",
    "| 2     | Pullover      |\n",
    "| 3     | Dress         |\n",
    "| 4     | Coat          |\n",
    "| 5     | Sandal        |\n",
    "| 6     | Shirt         |\n",
    "| 7     | Sneaker       |\n",
    "| 8     | Bag           |\n",
    "| 9     | Ankle boot    |\n",
    "\n",
    "* **Data Format**:\n",
    "  * Usually loaded as a **dictionary-like object**\n",
    "  * Keys: `'data'`, `'target'`, `'feature_names'`, `'target_names'`, `'DESCR'`, etc.\n",
    "  * `data` contains pixel values\n",
    "  * `target` contains class labels\n",
    "  * `frame` (if available) is a `pandas.DataFrame` with both\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Fashion-MNIST dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Min-Max normalization:\n",
    "Certainly! Here's the general **min-max normalization formula**, which scales a value $x$ from its original range $[x_{\\text{min}}, x_{\\text{max}}]$ to a new range $[a, b]$:\n",
    "\n",
    "$$\n",
    "x_{\\text{norm}} = a + \\left( \\frac{x - x_{\\text{min}}}{x_{\\text{max}} - x_{\\text{min}}} \\right) \\times (b - a)\n",
    "$$\n",
    "\n",
    "#### Special Case: Normalize to $[-1, 1]$\n",
    "\n",
    "$$\n",
    "x_{\\text{norm}} = \\frac{2(x - x_{\\text{min}})}{x_{\\text{max}} - x_{\\text{min}}} - 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Min-Max Normalization: From $[0, 255]$ to $[-1, 1]$\n",
    "\n",
    "To normalize a value $x \\in [0, 255]$ into the range $[-1, 1]$, you can use the **min-max normalization** formula:\n",
    "\n",
    "Sure! To normalize a value $x \\in [0, 255]$ into the range $[-1, 1]$, you use this **min-max normalization** formula:\n",
    "\n",
    "$$\n",
    "x_{\\text{norm}} = \\frac{2x}{255} - 1\n",
    "$$\n",
    "\n",
    "##### Explanation:\n",
    "\n",
    "This follows directly from:\n",
    "\n",
    "$$\n",
    "x_{\\text{norm}} = \\frac{2(x - x_{\\text{min}})}{x_{\\text{max}} - x_{\\text{min}}} - 1\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $x_{\\text{min}} = 0$\n",
    "* $x_{\\text{max}} = 255$\n",
    "\n",
    "Plug in:\n",
    "\n",
    "$$\n",
    "x_{\\text{norm}} = \\frac{2(x - 0)}{255 - 0} - 1 = \\frac{2x}{255} - 1\n",
    "$$\n",
    "\n",
    "This maps:\n",
    "\n",
    "* $x = 0 \\rightarrow -1$\n",
    "* $x = 127.5 \\rightarrow 0$\n",
    "* $x = 255 \\rightarrow +1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(filter_classes):\n",
    "    fashion_mnist = fetch_openml(\"Fashion-MNIST\", parser='auto')\n",
    "    x, y = fashion_mnist['data'], fashion_mnist['target'].astype(int)\n",
    "    # Remove classes\n",
    "    filtered_indices = np.isin(y, filter_classes)\n",
    "    x, y = x[filtered_indices].to_numpy(), y[filtered_indices]\n",
    "    # Normalize the pixels to be in [-1, +1] range\n",
    "    x = ((x / 255.) - .5) * 2\n",
    "    removed_class_count = 0\n",
    "    for i in range(10):  # Fix the labels\n",
    "        if i in filter_classes and removed_class_count != 0:\n",
    "            y[y == i] = i - removed_class_count\n",
    "        elif i not in filter_classes:\n",
    "            removed_class_count += 1\n",
    "    # Do the train-test split\n",
    "    return train_test_split(x, y, test_size=10_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Fashion-MNIST Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_encoder(y, num_labels):\n",
    "    one_hot = np.zeros(shape=(y.size, num_labels), dtype=int)\n",
    "    one_hot[np.arange(y.size), y] = 1\n",
    "    return one_hot\n",
    "\n",
    "class_names = {0: 'T-shirt/top', 1: 'Trouser', 2: 'Pullover',\n",
    "               3: 'Dress', 4: 'Coat', 5:  'Sandal', 6: 'Shirt',\n",
    "               7: 'Sneaker', 8: 'Bag', 9: 'Ankle boot'}\n",
    "\n",
    "kept_classes = [0, 1, 7] #t_shirt, trouser, sneaker\n",
    "\n",
    "x_train, x_test, y_train, y_test = get_data(kept_classes)\n",
    "\n",
    "# One-hot encode the target labels of the training set\n",
    "y_train = onehot_encoder(y_train, num_labels=len(kept_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.056: 100%|██████████| 30/30 [00:27<00:00,  1.08it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def plot_training(losses):\n",
    "    # Plot the loss\n",
    "    plt.plot(losses)\n",
    "    plt.title(\"Training loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "layers = [Linear(784, 50),\n",
    "          ReLU(),\n",
    "          Linear(50, 50),\n",
    "          ReLU(),\n",
    "          Linear(50, len(kept_classes)),\n",
    "          Softmax()]\n",
    "\n",
    "model = MLP(layers, CrossEntropy(), learning_rate=0.001) \n",
    "\n",
    "# Train the model\n",
    "losses = model.train(x_train, y_train, epochs=30, batch_size=64)\n",
    "\n",
    "plot_training(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
