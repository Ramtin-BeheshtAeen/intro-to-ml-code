{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement the Neural Network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input  = None \n",
    "        self.output = None\n",
    "\n",
    "    def __call__(self, input: np.ndarray) -> np.ndarray:\n",
    "        return self.forward(input)\n",
    "    \n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self, up_graad: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def step(self, learning_rate: float) -> None:\n",
    "        pass        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def __init__(self):\n",
    "        self.predicted = None\n",
    "        self.target    = None\n",
    "        self.loss      = None\n",
    "    \n",
    "    def __call__(self, prediction: np.ndarray, target: np.ndarray) -> float:\n",
    "        return self.forward(prediction, target)\n",
    "    \n",
    "    def forward(self, prediction:np.ndarray, target: np.ndarray) -> float:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self) -> np.ndarray:\n",
    "        raise NotImplementedError        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, layers: list[Layer], loss_func: Loss, learning_rate: float) -> None:\n",
    "        self.layers        = layers\n",
    "        self.loss_func     = loss_func\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def __call__(self, input: np.ndarray) -> np.ndarray:\n",
    "        return self.forward(input)\n",
    "    \n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" pass input through each layer sequentially \"\"\"\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        return input\n",
    "    \n",
    "    def loss(self, prediction: np.ndarray, target: np.ndarray) -> float:\n",
    "        \"\"\"Calculate Loss\"\"\"\n",
    "        return self.loss_func(prediction, target)\n",
    "    \n",
    "    def backward(self) -> None:\n",
    "        up_grad = self.loss_func.backward()\n",
    "        for layer in reversed(self.layers):\n",
    "            up_grad = layer.backward(up_grad)\n",
    "    \n",
    "    def update(self) -> None:\n",
    "        for layer in self.layers:\n",
    "            layer.step(self.learning_rate)\n",
    "    \n",
    "    def train(self, x_train: np.ndarray, y_train: np.ndarray, epochs: int, batch_size: int) -> np.ndarray:\n",
    "        losses = np.empty(epochs)\n",
    "        for epoch in (pbar := trange(epochs)):\n",
    "            running_loss = 0.0\n",
    "            for i in range(0, len(x_train), batch_size):\n",
    "                x_batch = x_train[i:i + batch_size]\n",
    "                y_batch = y_train[i:i + batch_size]\n",
    "\n",
    "                #Forward pass:\n",
    "                prediction = self.forward(x_batch)\n",
    "\n",
    "                #Compute loss:\n",
    "                running_loss += self.loss(prediction, y_batch) * batch_size\n",
    "\n",
    "                #Backward pass:\n",
    "                self.backward()\n",
    "\n",
    "                #Update the parameters:\n",
    "                self.update()\n",
    "            \n",
    "            #Normalize running loss by total number of samples:\n",
    "            running_loss /= len(x_train)\n",
    "            pbar.set_description(f\"Loss: {running_loss:.3f}\")\n",
    "            losses[epoch] = running_loss\n",
    "\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use MLP to solve a problem: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating the Fashion-MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = fetch_openml(\"Fashion-MNIST\", parser='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.utils._bunch.Bunch'>\n",
      "type of the fashion_mnist: <class 'sklearn.utils._bunch.Bunch'> \n",
      "\n",
      "keys of the fashion_mnist: dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])\n"
     ]
    }
   ],
   "source": [
    "print(type(fashion_mnist))\n",
    "print(f\"type of the fashion_mnist: {type(fashion_mnist)} \\n\")\n",
    "print(f\"keys of the fashion_mnist: {fashion_mnist.keys()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(fashion_mnist['data']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the dataset: (70000, 784)\n",
      "data of the fashion_mnist:    pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  pixel9  \\\n",
      "0       0       0       0       0       0       0       0       0       0   \n",
      "1       0       0       0       0       0       1       0       0       0   \n",
      "2       0       0       0       0       0       0       0       0       0   \n",
      "3       0       0       0       0       0       0       0       0      33   \n",
      "4       0       0       0       0       0       0       0       0       0   \n",
      "\n",
      "   pixel10  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
      "0        0  ...         0         0         0         0         0         0   \n",
      "1        0  ...       119       114       130        76         0         0   \n",
      "2       22  ...         0         0         1         0         0         0   \n",
      "3       96  ...         0         0         0         0         0         0   \n",
      "4        0  ...         0         0         0         0         0         0   \n",
      "\n",
      "   pixel781  pixel782  pixel783  pixel784  \n",
      "0         0         0         0         0  \n",
      "1         0         0         0         0  \n",
      "2         0         0         0         0  \n",
      "3         0         0         0         0  \n",
      "4         0         0         0         0  \n",
      "\n",
      "[5 rows x 784 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"shape of the dataset: {fashion_mnist['data'].shape}\")\n",
    "print(f\"data of the fashion_mnist: {fashion_mnist['data'].head()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing explained:\n",
    "- fashion_mnist['data'].shape[0] = 70000 → number of images (indexes from 0 to 69999).\n",
    "- fashion_mnist['data'].shape[1] = 784 → number of pixels in each image (indexes from 0 to 783)\n",
    "\n",
    "#### So:\n",
    "- When you do fashion_mnist['data'][index] — you are selecting the entire image at that row index (a 1D array of 784 pixels).\n",
    "\n",
    "- When you do fashion_mnist['data'][index][pixel] — you are selecting a single pixel value from the image at index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel1      0\n",
      "pixel2      0\n",
      "pixel3      0\n",
      "pixel4      0\n",
      "pixel5      0\n",
      "           ..\n",
      "pixel780    0\n",
      "pixel781    0\n",
      "pixel782    0\n",
      "pixel783    0\n",
      "pixel784    0\n",
      "Name: 0, Length: 784, dtype: int64\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Access first row (image 0) in the DataFrame\n",
    "first_image_row = fashion_mnist['data'].iloc[0]  # iloc for positional indexing\n",
    "\n",
    "print(first_image_row)  # This is a pandas Series with pixel columns\n",
    "\n",
    "# Access pixel 0 value in this row:\n",
    "print(first_image_row.iloc[0])\n",
    "\n",
    "# Or simply chain:\n",
    "print(fashion_mnist['data'].iloc[0].iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature names of the fashion_mnist: ['pixel1', 'pixel2', 'pixel3', 'pixel4', 'pixel5', 'pixel6', 'pixel7', 'pixel8', 'pixel9', 'pixel10', 'pixel11', 'pixel12', 'pixel13', 'pixel14', 'pixel15', 'pixel16', 'pixel17', 'pixel18', 'pixel19', 'pixel20', 'pixel21', 'pixel22', 'pixel23', 'pixel24', 'pixel25', 'pixel26', 'pixel27', 'pixel28', 'pixel29', 'pixel30', 'pixel31', 'pixel32', 'pixel33', 'pixel34', 'pixel35', 'pixel36', 'pixel37', 'pixel38', 'pixel39', 'pixel40', 'pixel41', 'pixel42', 'pixel43', 'pixel44', 'pixel45', 'pixel46', 'pixel47', 'pixel48', 'pixel49', 'pixel50', 'pixel51', 'pixel52', 'pixel53', 'pixel54', 'pixel55', 'pixel56', 'pixel57', 'pixel58', 'pixel59', 'pixel60', 'pixel61', 'pixel62', 'pixel63', 'pixel64', 'pixel65', 'pixel66', 'pixel67', 'pixel68', 'pixel69', 'pixel70', 'pixel71', 'pixel72', 'pixel73', 'pixel74', 'pixel75', 'pixel76', 'pixel77', 'pixel78', 'pixel79', 'pixel80', 'pixel81', 'pixel82', 'pixel83', 'pixel84', 'pixel85', 'pixel86', 'pixel87', 'pixel88', 'pixel89', 'pixel90', 'pixel91', 'pixel92', 'pixel93', 'pixel94', 'pixel95', 'pixel96', 'pixel97', 'pixel98', 'pixel99', 'pixel100', 'pixel101', 'pixel102', 'pixel103', 'pixel104', 'pixel105', 'pixel106', 'pixel107', 'pixel108', 'pixel109', 'pixel110', 'pixel111', 'pixel112', 'pixel113', 'pixel114', 'pixel115', 'pixel116', 'pixel117', 'pixel118', 'pixel119', 'pixel120', 'pixel121', 'pixel122', 'pixel123', 'pixel124', 'pixel125', 'pixel126', 'pixel127', 'pixel128', 'pixel129', 'pixel130', 'pixel131', 'pixel132', 'pixel133', 'pixel134', 'pixel135', 'pixel136', 'pixel137', 'pixel138', 'pixel139', 'pixel140', 'pixel141', 'pixel142', 'pixel143', 'pixel144', 'pixel145', 'pixel146', 'pixel147', 'pixel148', 'pixel149', 'pixel150', 'pixel151', 'pixel152', 'pixel153', 'pixel154', 'pixel155', 'pixel156', 'pixel157', 'pixel158', 'pixel159', 'pixel160', 'pixel161', 'pixel162', 'pixel163', 'pixel164', 'pixel165', 'pixel166', 'pixel167', 'pixel168', 'pixel169', 'pixel170', 'pixel171', 'pixel172', 'pixel173', 'pixel174', 'pixel175', 'pixel176', 'pixel177', 'pixel178', 'pixel179', 'pixel180', 'pixel181', 'pixel182', 'pixel183', 'pixel184', 'pixel185', 'pixel186', 'pixel187', 'pixel188', 'pixel189', 'pixel190', 'pixel191', 'pixel192', 'pixel193', 'pixel194', 'pixel195', 'pixel196', 'pixel197', 'pixel198', 'pixel199', 'pixel200', 'pixel201', 'pixel202', 'pixel203', 'pixel204', 'pixel205', 'pixel206', 'pixel207', 'pixel208', 'pixel209', 'pixel210', 'pixel211', 'pixel212', 'pixel213', 'pixel214', 'pixel215', 'pixel216', 'pixel217', 'pixel218', 'pixel219', 'pixel220', 'pixel221', 'pixel222', 'pixel223', 'pixel224', 'pixel225', 'pixel226', 'pixel227', 'pixel228', 'pixel229', 'pixel230', 'pixel231', 'pixel232', 'pixel233', 'pixel234', 'pixel235', 'pixel236', 'pixel237', 'pixel238', 'pixel239', 'pixel240', 'pixel241', 'pixel242', 'pixel243', 'pixel244', 'pixel245', 'pixel246', 'pixel247', 'pixel248', 'pixel249', 'pixel250', 'pixel251', 'pixel252', 'pixel253', 'pixel254', 'pixel255', 'pixel256', 'pixel257', 'pixel258', 'pixel259', 'pixel260', 'pixel261', 'pixel262', 'pixel263', 'pixel264', 'pixel265', 'pixel266', 'pixel267', 'pixel268', 'pixel269', 'pixel270', 'pixel271', 'pixel272', 'pixel273', 'pixel274', 'pixel275', 'pixel276', 'pixel277', 'pixel278', 'pixel279', 'pixel280', 'pixel281', 'pixel282', 'pixel283', 'pixel284', 'pixel285', 'pixel286', 'pixel287', 'pixel288', 'pixel289', 'pixel290', 'pixel291', 'pixel292', 'pixel293', 'pixel294', 'pixel295', 'pixel296', 'pixel297', 'pixel298', 'pixel299', 'pixel300', 'pixel301', 'pixel302', 'pixel303', 'pixel304', 'pixel305', 'pixel306', 'pixel307', 'pixel308', 'pixel309', 'pixel310', 'pixel311', 'pixel312', 'pixel313', 'pixel314', 'pixel315', 'pixel316', 'pixel317', 'pixel318', 'pixel319', 'pixel320', 'pixel321', 'pixel322', 'pixel323', 'pixel324', 'pixel325', 'pixel326', 'pixel327', 'pixel328', 'pixel329', 'pixel330', 'pixel331', 'pixel332', 'pixel333', 'pixel334', 'pixel335', 'pixel336', 'pixel337', 'pixel338', 'pixel339', 'pixel340', 'pixel341', 'pixel342', 'pixel343', 'pixel344', 'pixel345', 'pixel346', 'pixel347', 'pixel348', 'pixel349', 'pixel350', 'pixel351', 'pixel352', 'pixel353', 'pixel354', 'pixel355', 'pixel356', 'pixel357', 'pixel358', 'pixel359', 'pixel360', 'pixel361', 'pixel362', 'pixel363', 'pixel364', 'pixel365', 'pixel366', 'pixel367', 'pixel368', 'pixel369', 'pixel370', 'pixel371', 'pixel372', 'pixel373', 'pixel374', 'pixel375', 'pixel376', 'pixel377', 'pixel378', 'pixel379', 'pixel380', 'pixel381', 'pixel382', 'pixel383', 'pixel384', 'pixel385', 'pixel386', 'pixel387', 'pixel388', 'pixel389', 'pixel390', 'pixel391', 'pixel392', 'pixel393', 'pixel394', 'pixel395', 'pixel396', 'pixel397', 'pixel398', 'pixel399', 'pixel400', 'pixel401', 'pixel402', 'pixel403', 'pixel404', 'pixel405', 'pixel406', 'pixel407', 'pixel408', 'pixel409', 'pixel410', 'pixel411', 'pixel412', 'pixel413', 'pixel414', 'pixel415', 'pixel416', 'pixel417', 'pixel418', 'pixel419', 'pixel420', 'pixel421', 'pixel422', 'pixel423', 'pixel424', 'pixel425', 'pixel426', 'pixel427', 'pixel428', 'pixel429', 'pixel430', 'pixel431', 'pixel432', 'pixel433', 'pixel434', 'pixel435', 'pixel436', 'pixel437', 'pixel438', 'pixel439', 'pixel440', 'pixel441', 'pixel442', 'pixel443', 'pixel444', 'pixel445', 'pixel446', 'pixel447', 'pixel448', 'pixel449', 'pixel450', 'pixel451', 'pixel452', 'pixel453', 'pixel454', 'pixel455', 'pixel456', 'pixel457', 'pixel458', 'pixel459', 'pixel460', 'pixel461', 'pixel462', 'pixel463', 'pixel464', 'pixel465', 'pixel466', 'pixel467', 'pixel468', 'pixel469', 'pixel470', 'pixel471', 'pixel472', 'pixel473', 'pixel474', 'pixel475', 'pixel476', 'pixel477', 'pixel478', 'pixel479', 'pixel480', 'pixel481', 'pixel482', 'pixel483', 'pixel484', 'pixel485', 'pixel486', 'pixel487', 'pixel488', 'pixel489', 'pixel490', 'pixel491', 'pixel492', 'pixel493', 'pixel494', 'pixel495', 'pixel496', 'pixel497', 'pixel498', 'pixel499', 'pixel500', 'pixel501', 'pixel502', 'pixel503', 'pixel504', 'pixel505', 'pixel506', 'pixel507', 'pixel508', 'pixel509', 'pixel510', 'pixel511', 'pixel512', 'pixel513', 'pixel514', 'pixel515', 'pixel516', 'pixel517', 'pixel518', 'pixel519', 'pixel520', 'pixel521', 'pixel522', 'pixel523', 'pixel524', 'pixel525', 'pixel526', 'pixel527', 'pixel528', 'pixel529', 'pixel530', 'pixel531', 'pixel532', 'pixel533', 'pixel534', 'pixel535', 'pixel536', 'pixel537', 'pixel538', 'pixel539', 'pixel540', 'pixel541', 'pixel542', 'pixel543', 'pixel544', 'pixel545', 'pixel546', 'pixel547', 'pixel548', 'pixel549', 'pixel550', 'pixel551', 'pixel552', 'pixel553', 'pixel554', 'pixel555', 'pixel556', 'pixel557', 'pixel558', 'pixel559', 'pixel560', 'pixel561', 'pixel562', 'pixel563', 'pixel564', 'pixel565', 'pixel566', 'pixel567', 'pixel568', 'pixel569', 'pixel570', 'pixel571', 'pixel572', 'pixel573', 'pixel574', 'pixel575', 'pixel576', 'pixel577', 'pixel578', 'pixel579', 'pixel580', 'pixel581', 'pixel582', 'pixel583', 'pixel584', 'pixel585', 'pixel586', 'pixel587', 'pixel588', 'pixel589', 'pixel590', 'pixel591', 'pixel592', 'pixel593', 'pixel594', 'pixel595', 'pixel596', 'pixel597', 'pixel598', 'pixel599', 'pixel600', 'pixel601', 'pixel602', 'pixel603', 'pixel604', 'pixel605', 'pixel606', 'pixel607', 'pixel608', 'pixel609', 'pixel610', 'pixel611', 'pixel612', 'pixel613', 'pixel614', 'pixel615', 'pixel616', 'pixel617', 'pixel618', 'pixel619', 'pixel620', 'pixel621', 'pixel622', 'pixel623', 'pixel624', 'pixel625', 'pixel626', 'pixel627', 'pixel628', 'pixel629', 'pixel630', 'pixel631', 'pixel632', 'pixel633', 'pixel634', 'pixel635', 'pixel636', 'pixel637', 'pixel638', 'pixel639', 'pixel640', 'pixel641', 'pixel642', 'pixel643', 'pixel644', 'pixel645', 'pixel646', 'pixel647', 'pixel648', 'pixel649', 'pixel650', 'pixel651', 'pixel652', 'pixel653', 'pixel654', 'pixel655', 'pixel656', 'pixel657', 'pixel658', 'pixel659', 'pixel660', 'pixel661', 'pixel662', 'pixel663', 'pixel664', 'pixel665', 'pixel666', 'pixel667', 'pixel668', 'pixel669', 'pixel670', 'pixel671', 'pixel672', 'pixel673', 'pixel674', 'pixel675', 'pixel676', 'pixel677', 'pixel678', 'pixel679', 'pixel680', 'pixel681', 'pixel682', 'pixel683', 'pixel684', 'pixel685', 'pixel686', 'pixel687', 'pixel688', 'pixel689', 'pixel690', 'pixel691', 'pixel692', 'pixel693', 'pixel694', 'pixel695', 'pixel696', 'pixel697', 'pixel698', 'pixel699', 'pixel700', 'pixel701', 'pixel702', 'pixel703', 'pixel704', 'pixel705', 'pixel706', 'pixel707', 'pixel708', 'pixel709', 'pixel710', 'pixel711', 'pixel712', 'pixel713', 'pixel714', 'pixel715', 'pixel716', 'pixel717', 'pixel718', 'pixel719', 'pixel720', 'pixel721', 'pixel722', 'pixel723', 'pixel724', 'pixel725', 'pixel726', 'pixel727', 'pixel728', 'pixel729', 'pixel730', 'pixel731', 'pixel732', 'pixel733', 'pixel734', 'pixel735', 'pixel736', 'pixel737', 'pixel738', 'pixel739', 'pixel740', 'pixel741', 'pixel742', 'pixel743', 'pixel744', 'pixel745', 'pixel746', 'pixel747', 'pixel748', 'pixel749', 'pixel750', 'pixel751', 'pixel752', 'pixel753', 'pixel754', 'pixel755', 'pixel756', 'pixel757', 'pixel758', 'pixel759', 'pixel760', 'pixel761', 'pixel762', 'pixel763', 'pixel764', 'pixel765', 'pixel766', 'pixel767', 'pixel768', 'pixel769', 'pixel770', 'pixel771', 'pixel772', 'pixel773', 'pixel774', 'pixel775', 'pixel776', 'pixel777', 'pixel778', 'pixel779', 'pixel780', 'pixel781', 'pixel782', 'pixel783', 'pixel784']\n",
      "target names of the fashion_mnist: ['class']\n"
     ]
    }
   ],
   "source": [
    "print(f\"feature names of the fashion_mnist: {fashion_mnist['feature_names']}\")\n",
    "print(f\"target names of the fashion_mnist: {fashion_mnist['target_names']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "1\n",
      "i: class\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "#The dataset’s target_names list only contains the name of the target column, which is \"class\".\n",
    "target_names = fashion_mnist['target_names']\n",
    "\n",
    "print(type(target_names))\n",
    "\n",
    "print(len(target_names))\n",
    "\n",
    "for i in target_names:\n",
    "    print(f\"i: {i}\")\n",
    "    print(type(i))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLot the MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First image pixel array shape: (784,)\n",
      "First pixel value of first image: 0\n",
      "Label number: 9\n",
      "Label name: Ankle boot\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Suppose fashion_mnist is loaded as a dictionary with pandas DataFrames/Series\n",
    "# Example class names mapping:\n",
    "class_names = {\n",
    "    0: \"T-shirt/top\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle boot\"\n",
    "}\n",
    "\n",
    "index = 0  # Index of the image to examine\n",
    "\n",
    "# Access image pixels as pandas Series and convert to numpy array\n",
    "image_pixels = fashion_mnist['data'].iloc[index].values\n",
    "\n",
    "# Access first pixel value\n",
    "first_pixel_value = image_pixels[0]\n",
    "\n",
    "# Access label\n",
    "label = fashion_mnist['target'].iloc[index]\n",
    "\n",
    "print(f\"First image pixel array shape: {image_pixels.shape}\")  # Should be (784,)\n",
    "print(f\"First pixel value of first image: {first_pixel_value}\")\n",
    "print(f\"Label number: {label}\")\n",
    "print(f\"Label name: {class_names[int(label)]}\")\n",
    "\n",
    "# Reshape to 28x28 for visualization\n",
    "image_2d = image_pixels.reshape(28, 28)\n",
    "\n",
    "# Plot the image\n",
    "plt.imshow(image_2d, cmap='gray')\n",
    "plt.title(f\"Class: {class_names[int(label)]}\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "summary of the **Fashion MNIST dataset** :\n",
    "\n",
    "**Fashion MNIST Dataset – Full Summary**\n",
    "\n",
    "* **What it is**:\n",
    "  A benchmark dataset for machine learning, used to train and evaluate image classification models on clothing item recognition.\n",
    "\n",
    "* **Total Samples**: **70,000 grayscale images**\n",
    "\n",
    "  * **60,000 training samples**\n",
    "  * **10,000 test samples**\n",
    "\n",
    "* **Image Details**:\n",
    "\n",
    "  * Each image is **28 × 28 pixels** = **784 pixels total**\n",
    "  * Stored as a **flattened 1D array of 784 values**\n",
    "  * Each pixel is a **grayscale intensity**: value between **0 (black)** and **255 (white)**\n",
    "\n",
    "* **Features**:\n",
    "\n",
    "  * Named `pixel1`, `pixel2`, ..., `pixel784`\n",
    "  * Represent the brightness level of each pixel\n",
    "  * No color (only black and white shades)\n",
    "\n",
    "* **Target Labels**:\n",
    "  One label per image, from **0 to 9**, representing the clothing type\n",
    "\n",
    "| Label | Clothing Item |\n",
    "| ----- | ------------- |\n",
    "| 0     | T-shirt/top   |\n",
    "| 1     | Trouser       |\n",
    "| 2     | Pullover      |\n",
    "| 3     | Dress         |\n",
    "| 4     | Coat          |\n",
    "| 5     | Sandal        |\n",
    "| 6     | Shirt         |\n",
    "| 7     | Sneaker       |\n",
    "| 8     | Bag           |\n",
    "| 9     | Ankle boot    |\n",
    "\n",
    "* **Data Format**:\n",
    "  * Usually loaded as a **dictionary-like object**\n",
    "  * Keys: `'data'`, `'target'`, `'feature_names'`, `'target_names'`, `'DESCR'`, etc.\n",
    "  * `data` contains pixel values\n",
    "  * `target` contains class labels\n",
    "  * `frame` (if available) is a `pandas.DataFrame` with both\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Fashion-MNIST dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Min-Max normalization:\n",
    "Certainly! Here's the general **min-max normalization formula**, which scales a value $x$ from its original range $[x_{\\text{min}}, x_{\\text{max}}]$ to a new range $[a, b]$:\n",
    "\n",
    "$$\n",
    "x_{\\text{norm}} = a + \\left( \\frac{x - x_{\\text{min}}}{x_{\\text{max}} - x_{\\text{min}}} \\right) \\times (b - a)\n",
    "$$\n",
    "\n",
    "#### Special Case: Normalize to $[-1, 1]$\n",
    "\n",
    "$$\n",
    "x_{\\text{norm}} = \\frac{2(x - x_{\\text{min}})}{x_{\\text{max}} - x_{\\text{min}}} - 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Min-Max Normalization: From $[0, 255]$ to $[-1, 1]$\n",
    "\n",
    "To normalize a value $x \\in [0, 255]$ into the range $[-1, 1]$, you can use the **min-max normalization** formula:\n",
    "\n",
    "Sure! To normalize a value $x \\in [0, 255]$ into the range $[-1, 1]$, you use this **min-max normalization** formula:\n",
    "\n",
    "$$\n",
    "x_{\\text{norm}} = \\frac{2x}{255} - 1\n",
    "$$\n",
    "\n",
    "##### Explanation:\n",
    "\n",
    "This follows directly from:\n",
    "\n",
    "$$\n",
    "x_{\\text{norm}} = \\frac{2(x - x_{\\text{min}})}{x_{\\text{max}} - x_{\\text{min}}} - 1\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $x_{\\text{min}} = 0$\n",
    "* $x_{\\text{max}} = 255$\n",
    "\n",
    "Plug in:\n",
    "\n",
    "$$\n",
    "x_{\\text{norm}} = \\frac{2(x - 0)}{255 - 0} - 1 = \\frac{2x}{255} - 1\n",
    "$$\n",
    "\n",
    "This maps:\n",
    "\n",
    "* $x = 0 \\rightarrow -1$\n",
    "* $x = 127.5 \\rightarrow 0$\n",
    "* $x = 255 \\rightarrow +1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(filter_classes):\n",
    "    fashion_mnist = fetch_openml(\"Fashion-MNIST\", parser='auto')\n",
    "    x, y = fashion_mnist['data'], fashion_mnist['target'].astype(int)\n",
    "    # Remove classes\n",
    "    filtered_indices = np.isin(y, filter_classes)\n",
    "    x, y = x[filtered_indices].to_numpy(), y[filtered_indices]\n",
    "    # Normalize the pixels to be in [-1, +1] range\n",
    "    x = ((x / 255.) - .5) * 2\n",
    "    removed_class_count = 0\n",
    "    for i in range(10):  # Fix the labels\n",
    "        if i in filter_classes and removed_class_count != 0:\n",
    "            y[y == i] = i - removed_class_count\n",
    "        elif i not in filter_classes:\n",
    "            removed_class_count += 1\n",
    "    # Do the train-test split\n",
    "    return train_test_split(x, y, test_size=10_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Fashion-MNIST Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_encoder(y, num_labels):\n",
    "    one_hot = np.zeros(shape=(y.size, num_labels), dtype=int)\n",
    "    one_hot[np.arange(y.size), y] = 1\n",
    "    return one_hot\n",
    "\n",
    "class_names = {0: 'T-shirt/top', 1: 'Trouser', 2: 'Pullover',\n",
    "               3: 'Dress', 4: 'Coat', 5:  'Sandal', 6: 'Shirt',\n",
    "               7: 'Sneaker', 8: 'Bag', 9: 'Ankle boot'}\n",
    "\n",
    "kept_classes = [0, 1, 7] #t_shirt, trouser, sneaker\n",
    "\n",
    "x_train, x_test, y_train, y_test = get_data(kept_classes)\n",
    "\n",
    "# One-hot encode the target labels of the training set\n",
    "y_train = onehot_encoder(y_train, num_labels=len(kept_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
